{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "293d2ba3",
   "metadata": {},
   "source": [
    "# DeepBach notebook\n",
    "\n",
    "Implementation of DeepBach code in notebook and Google Colab format. See original project for more details see original project description and repository https://github.com/Ghadjeres/DeepBach\n",
    "\n",
    "Can be run both locally and on Google Colab"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c_B35A3T8BX",
   "metadata": {
    "id": "0c_B35A3T8BX"
   },
   "source": [
    "## Requirements and imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "IJsCgIAPkjm-",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IJsCgIAPkjm-",
    "outputId": "f44c5c0b-e216-4539-e3d4-acc53a1866da"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (4.65.0)\n",
      "Collecting music21==5.5.0\n",
      "  Downloading music21-5.5.0.tar.gz (18.5 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.5/18.5 MB\u001b[0m \u001b[31m79.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "Building wheels for collected packages: music21\n",
      "  Building wheel for music21 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for music21: filename=music21-5.5.0-py3-none-any.whl size=21451894 sha256=b4e01dae18f7121efbdd123e6f75f2a145c99afeab0e3cbfb26855708cf1ce76\n",
      "  Stored in directory: /root/.cache/pip/wheels/99/ff/03/582aca7d70f75ef320d87d8a7385bc6698b24e2941ed050b92\n",
      "Successfully built music21\n",
      "Installing collected packages: music21\n",
      "  Attempting uninstall: music21\n",
      "    Found existing installation: music21 8.1.0\n",
      "    Uninstalling music21-8.1.0:\n",
      "      Successfully uninstalled music21-8.1.0\n",
      "Successfully installed music21-5.5.0\n"
     ]
    }
   ],
   "source": [
    "!pip install tqdm \\\n",
    "music21==5.5.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "UUZWiq7bkBjD",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UUZWiq7bkBjD",
    "outputId": "50ede236-9425-4c1a-f8a5-206d8eaba8e6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount(\"/content/drive\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "86aaff0d",
   "metadata": {
    "id": "86aaff0d"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "music21: Certain music21 functions might need the optional package scipy;\n",
      "                  if you run into errors, install it by following the instructions at\n",
      "                  http://mit.edu/music21/doc/installing/installAdditional.html\n"
     ]
    }
   ],
   "source": [
    "from abc import ABC, abstractmethod\n",
    "import os\n",
    "from itertools import islice\n",
    "import random\n",
    "import datetime as dt\n",
    "\n",
    "\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from torch import nn, optim\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from music21 import analysis, stream, meter\n",
    "from music21 import note, harmony, expressions\n",
    "from music21 import interval, stream\n",
    "import music21\n",
    "from IPython.display import display, Audio\n",
    "\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0c0a36a9",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "id": "0c0a36a9",
    "outputId": "927a79c9-03c3-4a05-e253-e0daf0867f8f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/mylh/Projects/music_ai/DreamBach'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "IN_COLAB = 'google.colab' in sys.modules\n",
    "if IN_COLAB:\n",
    "    PROJECT_ROOT = '/content/drive/MyDrive/DreamBach/'\n",
    "else:\n",
    "    PROJECT_ROOT = os.getcwd()\n",
    "DATASET_CACHE_DIR = os.path.join(PROJECT_ROOT, 'dataset_cache/')\n",
    "MODELS_SAVE_DIR = os.path.join(PROJECT_ROOT, 'models/')\n",
    "MIDI_SAVE_DIR = os.path.join(PROJECT_ROOT, 'midi/')\n",
    "os.makedirs(DATASET_CACHE_DIR, exist_ok=True)\n",
    "os.makedirs(MODELS_SAVE_DIR, exist_ok=True)\n",
    "os.makedirs(MIDI_SAVE_DIR, exist_ok=True)\n",
    "PROJECT_ROOT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "239ebb60",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "239ebb60",
    "outputId": "b7223102-62d6-40aa-df35-5bd77ddafc85"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "126fabde",
   "metadata": {
    "id": "126fabde"
   },
   "source": [
    "### Helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "08ba9f72",
   "metadata": {
    "id": "08ba9f72"
   },
   "outputs": [],
   "source": [
    "def cuda_variable(tensor):\n",
    "    if torch.cuda.is_available():\n",
    "        return Variable(tensor.cuda())\n",
    "    else:\n",
    "        return Variable(tensor)\n",
    "\n",
    "\n",
    "def to_numpy(variable: Variable):\n",
    "    if torch.cuda.is_available():\n",
    "        return variable.data.cpu().numpy()\n",
    "    else:\n",
    "        return variable.data.numpy()\n",
    "\n",
    "\n",
    "def init_hidden(num_layers, batch_size, lstm_hidden_size):\n",
    "    hidden = (\n",
    "        cuda_variable(\n",
    "            torch.randn(num_layers, batch_size, lstm_hidden_size)),\n",
    "        cuda_variable(\n",
    "            torch.randn(num_layers, batch_size, lstm_hidden_size))\n",
    "    )\n",
    "    return hidden\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9dccf32e",
   "metadata": {
    "id": "9dccf32e"
   },
   "outputs": [],
   "source": [
    "def mask_entry(tensor, entry_index, dim):\n",
    "    \"\"\"\n",
    "    Masks entry entry_index on dim dim\n",
    "    similar to\n",
    "    torch.cat((\ttensor[ :entry_index],\ttensor[ entry_index + 1 :], 0)\n",
    "    but on another dimension\n",
    "    :param tensor:\n",
    "    :param entry_index:\n",
    "    :param dim:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    idx = [i for i in range(tensor.size(dim)) if not i == entry_index]\n",
    "    idx = cuda_variable(torch.LongTensor(idx))\n",
    "    tensor = tensor.index_select(dim, idx)\n",
    "    return tensor\n",
    "\n",
    "\n",
    "def reverse_tensor(tensor, dim):\n",
    "    \"\"\"\n",
    "    Do tensor[:, ... ,  -1::-1, :] along dim dim\n",
    "    :param tensor:\n",
    "    :param dim:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    idx = [i for i in range(tensor.size(dim) - 1, -1, -1)]\n",
    "    idx = cuda_variable(torch.LongTensor(idx))\n",
    "    tensor = tensor.index_select(dim, idx)\n",
    "    return tensor\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "_yDYVjHYrDZr",
   "metadata": {
    "id": "_yDYVjHYrDZr"
   },
   "source": [
    "## Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "16a0450c",
   "metadata": {
    "cellView": "form",
    "id": "16a0450c"
   },
   "outputs": [],
   "source": [
    "#@title Metadata Classes\n",
    "\"\"\"\n",
    "Metadata classes\n",
    "\"\"\"\n",
    "class Metadata:\n",
    "    def __init__(self):\n",
    "        self.num_values = None\n",
    "        self.is_global = None\n",
    "        self.name = None\n",
    "\n",
    "    def get_index(self, value):\n",
    "        # trick with the 0 value\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def get_value(self, index):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def evaluate(self, chorale, subdivision):\n",
    "        \"\"\"\n",
    "        takes a music21 chorale as input and the number of subdivisions per beat\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def generate(self, length):\n",
    "        raise NotImplementedError\n",
    "\n",
    "\n",
    "class IsPlayingMetadata(Metadata):\n",
    "    def __init__(self, voice_index, min_num_ticks):\n",
    "        \"\"\"\n",
    "        Metadata that indicates if a voice is playing\n",
    "        Voice i is considered to be muted if more than 'min_num_ticks' contiguous\n",
    "        ticks contain a rest.\n",
    "\n",
    "\n",
    "        :param voice_index: index of the voice to take into account\n",
    "        :param min_num_ticks: minimum length in ticks for a rest to be taken\n",
    "        into account in the metadata\n",
    "        \"\"\"\n",
    "        super(IsPlayingMetadata, self).__init__()\n",
    "        self.min_num_ticks = min_num_ticks\n",
    "        self.voice_index = voice_index\n",
    "        self.is_global = False\n",
    "        self.num_values = 2\n",
    "        self.name = 'isplaying'\n",
    "\n",
    "    def get_index(self, value):\n",
    "        return int(value)\n",
    "\n",
    "    def get_value(self, index):\n",
    "        return bool(index)\n",
    "\n",
    "    def evaluate(self, chorale, subdivision):\n",
    "        \"\"\"\n",
    "        takes a music21 chorale as input\n",
    "        \"\"\"\n",
    "        length = int(chorale.duration.quarterLength * subdivision)\n",
    "        metadatas = np.ones(shape=(length,))\n",
    "        part = chorale.parts[self.voice_index]\n",
    "\n",
    "        for note_or_rest in part.notesAndRests:\n",
    "            is_playing = True\n",
    "            if note_or_rest.isRest:\n",
    "                if note_or_rest.quarterLength * subdivision >= self.min_num_ticks:\n",
    "                    is_playing = False\n",
    "            # these should be integer values\n",
    "            start_tick = note_or_rest.offset * subdivision\n",
    "            end_tick = start_tick + note_or_rest.quarterLength * subdivision\n",
    "            metadatas[start_tick:end_tick] = self.get_index(is_playing)\n",
    "        return metadatas\n",
    "\n",
    "    def generate(self, length):\n",
    "        return np.ones(shape=(length,))\n",
    "\n",
    "\n",
    "class TickMetadata(Metadata):\n",
    "    \"\"\"\n",
    "    Metadata class that tracks on which subdivision of the beat we are on\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, subdivision):\n",
    "        super(TickMetadata, self).__init__()\n",
    "        self.is_global = False\n",
    "        self.num_values = subdivision\n",
    "        self.name = 'tick'\n",
    "\n",
    "    def get_index(self, value):\n",
    "        return value\n",
    "\n",
    "    def get_value(self, index):\n",
    "        return index\n",
    "\n",
    "    def evaluate(self, chorale, subdivision):\n",
    "        assert subdivision == self.num_values\n",
    "        # suppose all pieces start on a beat\n",
    "        length = int(chorale.duration.quarterLength * subdivision)\n",
    "        return np.array(list(map(\n",
    "            lambda x: x % self.num_values,\n",
    "            range(length)\n",
    "        )))\n",
    "\n",
    "    def generate(self, length):\n",
    "        return np.array(list(map(\n",
    "            lambda x: x % self.num_values,\n",
    "            range(length)\n",
    "        )))\n",
    "\n",
    "\n",
    "class ModeMetadata(Metadata):\n",
    "    \"\"\"\n",
    "    Metadata class that indicates the current mode of the melody\n",
    "    can be major, minor or other\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super(ModeMetadata, self).__init__()\n",
    "        self.is_global = False\n",
    "        self.num_values = 3  # major, minor or other\n",
    "        self.name = 'mode'\n",
    "\n",
    "    def get_index(self, value):\n",
    "        if value == 'major':\n",
    "            return 1\n",
    "        if value == 'minor':\n",
    "            return 2\n",
    "        return 0\n",
    "\n",
    "    def get_value(self, index):\n",
    "        if index == 1:\n",
    "            return 'major'\n",
    "        if index == 2:\n",
    "            return 'minor'\n",
    "        return 'other'\n",
    "\n",
    "    def evaluate(self, chorale, subdivision):\n",
    "        # todo add measures when in midi\n",
    "        # init key analyzer\n",
    "        ka = analysis.floatingKey.KeyAnalyzer(chorale)\n",
    "        res = ka.run()\n",
    "\n",
    "        measure_offset_map = chorale.parts[0].measureOffsetMap()\n",
    "        length = int(chorale.duration.quarterLength * subdivision)  # in 16th notes\n",
    "\n",
    "        modes = np.zeros((length,))\n",
    "\n",
    "        measure_index = -1\n",
    "        for time_index in range(length):\n",
    "            beat_index = time_index / subdivision\n",
    "            if beat_index in measure_offset_map:\n",
    "                measure_index += 1\n",
    "                modes[time_index] = self.get_index(res[measure_index].mode)\n",
    "\n",
    "        return np.array(modes, dtype=np.int32)\n",
    "\n",
    "    def generate(self, length):\n",
    "        return np.full((length,), self.get_index('major'))\n",
    "\n",
    "\n",
    "class KeyMetadata(Metadata):\n",
    "    \"\"\"\n",
    "    Metadata class that indicates in which key we are\n",
    "    Only returns the number of sharps or flats\n",
    "    Does not distinguish a key from its relative key\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, window_size=4):\n",
    "        super(KeyMetadata, self).__init__()\n",
    "        self.window_size = window_size\n",
    "        self.is_global = False\n",
    "        self.num_max_sharps = 7\n",
    "        self.num_values = 16\n",
    "        self.name = 'key'\n",
    "\n",
    "    def get_index(self, value):\n",
    "        \"\"\"\n",
    "\n",
    "        :param value: number of sharps (between -7 and +7)\n",
    "        :return: index in the representation\n",
    "        \"\"\"\n",
    "        return value + self.num_max_sharps + 1\n",
    "\n",
    "    def get_value(self, index):\n",
    "        \"\"\"\n",
    "\n",
    "        :param index:  index (between 0 and self.num_values); 0 is unused (no constraint)\n",
    "        :return: true number of sharps (between -7 and 7)\n",
    "        \"\"\"\n",
    "        return index - 1 - self.num_max_sharps\n",
    "\n",
    "    def evaluate(self, chorale, subdivision):\n",
    "        # init key analyzer\n",
    "        # we must add measures by hand for the case when we are parsing midi files\n",
    "        chorale_with_measures = stream.Score()\n",
    "        for part in chorale.parts:\n",
    "            chorale_with_measures.append(part.makeMeasures())\n",
    "\n",
    "        ka = analysis.floatingKey.KeyAnalyzer(chorale_with_measures)\n",
    "        ka.windowSize = self.window_size\n",
    "        res = ka.run()\n",
    "\n",
    "        measure_offset_map = chorale_with_measures.parts.measureOffsetMap()\n",
    "        length = int(chorale.duration.quarterLength * subdivision)  # in 16th notes\n",
    "\n",
    "        key_signatures = np.zeros((length,))\n",
    "\n",
    "        measure_index = -1\n",
    "        for time_index in range(length):\n",
    "            beat_index = time_index / subdivision\n",
    "            if beat_index in measure_offset_map:\n",
    "                measure_index += 1\n",
    "                if measure_index == len(res):\n",
    "                    measure_index -= 1\n",
    "\n",
    "            key_signatures[time_index] = self.get_index(res[measure_index].sharps)\n",
    "        return np.array(key_signatures, dtype=np.int32)\n",
    "\n",
    "    def generate(self, length):\n",
    "        return np.full((length,), self.get_index(0))\n",
    "\n",
    "\n",
    "class FermataMetadata(Metadata):\n",
    "    \"\"\"\n",
    "    Metadata class which indicates if a fermata is on the current note\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super(FermataMetadata, self).__init__()\n",
    "        self.is_global = False\n",
    "        self.num_values = 2\n",
    "        self.name = 'fermata'\n",
    "\n",
    "    def get_index(self, value):\n",
    "        # possible values are 1 and 0, thus value = index\n",
    "        return value\n",
    "\n",
    "    def get_value(self, index):\n",
    "        # possible values are 1 and 0, thus value = index\n",
    "        return index\n",
    "\n",
    "    def evaluate(self, chorale, subdivision):\n",
    "        part = chorale.parts[0]\n",
    "        length = int(part.duration.quarterLength * subdivision)  # in 16th notes\n",
    "        list_notes = part.flat.notes\n",
    "        num_notes = len(list_notes)\n",
    "        j = 0\n",
    "        i = 0\n",
    "        fermatas = np.zeros((length,))\n",
    "        while i < length:\n",
    "            if j < num_notes - 1:\n",
    "                if list_notes[j + 1].offset > i / subdivision:\n",
    "\n",
    "                    if len(list_notes[j].expressions) == 1:\n",
    "                        fermata = True\n",
    "                    else:\n",
    "                        fermata = False\n",
    "                    fermatas[i] = fermata\n",
    "                    i += 1\n",
    "                else:\n",
    "                    j += 1\n",
    "            else:\n",
    "                if len(list_notes[j].expressions) == 1:\n",
    "                    fermata = True\n",
    "                else:\n",
    "                    fermata = False\n",
    "\n",
    "                fermatas[i] = fermata\n",
    "                i += 1\n",
    "        return np.array(fermatas, dtype=np.int32)\n",
    "\n",
    "    def generate(self, length):\n",
    "        # fermata every 2 bars\n",
    "        return np.array([1 if i % 32 >= 28 else 0\n",
    "                         for i in range(length)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d93f037a",
   "metadata": {
    "cellView": "form",
    "id": "d93f037a"
   },
   "outputs": [],
   "source": [
    "#@title Dataset Helpers\n",
    "# constants\n",
    "SLUR_SYMBOL = '__'\n",
    "START_SYMBOL = 'START'\n",
    "END_SYMBOL = 'END'\n",
    "REST_SYMBOL = 'rest'\n",
    "OUT_OF_RANGE = 'OOR'\n",
    "PAD_SYMBOL = 'XX'\n",
    "\n",
    "\n",
    "def standard_name(note_or_rest, voice_range=None):\n",
    "    \"\"\"\n",
    "    Convert music21 objects to str\n",
    "    :param note_or_rest:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    if isinstance(note_or_rest, note.Note):\n",
    "        if voice_range is not None:\n",
    "            min_pitch, max_pitch = voice_range\n",
    "            pitch = note_or_rest.pitch.midi\n",
    "            if pitch < min_pitch or pitch > max_pitch:\n",
    "                return OUT_OF_RANGE\n",
    "        return note_or_rest.nameWithOctave\n",
    "    if isinstance(note_or_rest, note.Rest):\n",
    "        return note_or_rest.name  # == 'rest' := REST_SYMBOL\n",
    "    if isinstance(note_or_rest, str):\n",
    "        return note_or_rest\n",
    "\n",
    "    if isinstance(note_or_rest, harmony.ChordSymbol):\n",
    "        return note_or_rest.figure\n",
    "    if isinstance(note_or_rest, expressions.TextExpression):\n",
    "        return note_or_rest.content\n",
    "\n",
    "\n",
    "def standard_note(note_or_rest_string):\n",
    "    \"\"\"\n",
    "    Convert str representing a music21 object to this object\n",
    "    :param note_or_rest_string:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    if note_or_rest_string == 'rest':\n",
    "        return note.Rest()\n",
    "    # treat other additional symbols as rests\n",
    "    elif (note_or_rest_string == END_SYMBOL\n",
    "          or\n",
    "          note_or_rest_string == START_SYMBOL\n",
    "          or\n",
    "          note_or_rest_string == PAD_SYMBOL):\n",
    "        # print('Warning: Special symbol is used in standard_note')\n",
    "        return note.Rest()\n",
    "    elif note_or_rest_string == SLUR_SYMBOL:\n",
    "        # print('Warning: SLUR_SYMBOL used in standard_note')\n",
    "        return note.Rest()\n",
    "    elif note_or_rest_string == OUT_OF_RANGE:\n",
    "        # print('Warning: OUT_OF_RANGE used in standard_note')\n",
    "        return note.Rest()\n",
    "    else:\n",
    "        return note.Note(note_or_rest_string)\n",
    "\n",
    "\n",
    "class ShortChoraleIteratorGen:\n",
    "    \"\"\"\n",
    "    Class used for debugging\n",
    "    when called, it returns an iterator over 3 Bach chorales,\n",
    "    similar to music21.corpus.chorales.Iterator()\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def __call__(self):\n",
    "        it = (\n",
    "            chorale\n",
    "            for chorale in\n",
    "            islice(music21.corpus.chorales.Iterator(), 3)\n",
    "        )\n",
    "        return it.__iter__()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7ba71b3a",
   "metadata": {
    "id": "7ba71b3a"
   },
   "outputs": [],
   "source": [
    "#@title Music Dataset\n",
    "class MusicDataset(ABC):\n",
    "    \"\"\"\n",
    "    Abstract Base Class for music datasets\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, cache_dir):\n",
    "        self._tensor_dataset = None\n",
    "        self.cache_dir = cache_dir\n",
    "\n",
    "    @abstractmethod\n",
    "    def iterator_gen(self):\n",
    "        \"\"\"\n",
    "\n",
    "        return: Iterator over the dataset\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def make_tensor_dataset(self):\n",
    "        \"\"\"\n",
    "\n",
    "        :return: TensorDataset\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def get_score_tensor(self, score):\n",
    "        \"\"\"\n",
    "\n",
    "        :param score: music21 score object\n",
    "        :return: torch tensor, with the score representation\n",
    "                 as a tensor\n",
    "        \"\"\"\n",
    "        pass\n",
    "    \n",
    "    @abstractmethod\n",
    "    def get_metadata_tensor(self, score):\n",
    "        \"\"\"\n",
    "\n",
    "        :param score: music21 score object\n",
    "        :return: torch tensor, with the metadata representation\n",
    "                 as a tensor\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def transposed_score_and_metadata_tensors(self, score, semi_tone):\n",
    "        \"\"\"\n",
    "\n",
    "        :param score: music21 score object\n",
    "        :param semi-tone: int, +12 to -12, semitones to transpose \n",
    "        :return: Transposed score shifted by the semi-tone\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def extract_score_tensor_with_padding(self, \n",
    "                                          tensor_score, \n",
    "                                          start_tick, \n",
    "                                          end_tick):\n",
    "        \"\"\"\n",
    "\n",
    "        :param tensor_score: torch tensor containing the score representation\n",
    "        :param start_tick:\n",
    "        :param end_tick:\n",
    "        :return: tensor_score[:, start_tick: end_tick]\n",
    "        with padding if necessary\n",
    "        i.e. if start_tick < 0 or end_tick > tensor_score length\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def extract_metadata_with_padding(self, \n",
    "                                      tensor_metadata,\n",
    "                                      start_tick, \n",
    "                                      end_tick):\n",
    "        \"\"\"\n",
    "\n",
    "        :param tensor_metadata: torch tensor containing metadata\n",
    "        :param start_tick:\n",
    "        :param end_tick:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def empty_score_tensor(self, score_length):\n",
    "        \"\"\"\n",
    "        \n",
    "        :param score_length: int, length of the score in ticks\n",
    "        :return: torch long tensor, initialized with start indices \n",
    "        \"\"\"\n",
    "        pass \n",
    "\n",
    "    @abstractmethod\n",
    "    def random_score_tensor(self, score_length):\n",
    "        \"\"\"\n",
    "\n",
    "        :param score_length: int, length of the score in ticks\n",
    "        :return: torch long tensor, initialized with random indices\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def tensor_to_score(self, tensor_score):\n",
    "        \"\"\"\n",
    "\n",
    "        :param tensor_score: torch tensor, tensor representation\n",
    "                             of the score\n",
    "        :return: music21 score object\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    @property\n",
    "    def tensor_dataset(self):\n",
    "        \"\"\"\n",
    "        Loads or computes TensorDataset\n",
    "        :return: TensorDataset\n",
    "        \"\"\"\n",
    "        if self._tensor_dataset is None:\n",
    "            if self.tensor_dataset_is_cached():\n",
    "                print(f'Loading TensorDataset for {self.__repr__()}')\n",
    "                self._tensor_dataset = torch.load(self.tensor_dataset_filepath)\n",
    "            else:\n",
    "                print(f'Creating {self.__repr__()} TensorDataset'\n",
    "                      f' since it is not cached')\n",
    "                self._tensor_dataset = self.make_tensor_dataset()\n",
    "                torch.save(self._tensor_dataset, self.tensor_dataset_filepath)\n",
    "                print(f'TensorDataset for {self.__repr__()} '\n",
    "                      f'saved in {self.tensor_dataset_filepath}')\n",
    "        return self._tensor_dataset\n",
    "\n",
    "    @tensor_dataset.setter\n",
    "    def tensor_dataset(self, value):\n",
    "        self._tensor_dataset = value\n",
    "\n",
    "    def tensor_dataset_is_cached(self):\n",
    "        return os.path.exists(self.tensor_dataset_filepath)\n",
    "\n",
    "    @property\n",
    "    def tensor_dataset_filepath(self):\n",
    "        tensor_datasets_cache_dir = os.path.join(\n",
    "            self.cache_dir,\n",
    "            'tensor_datasets')\n",
    "        if not os.path.exists(tensor_datasets_cache_dir):\n",
    "            os.mkdir(tensor_datasets_cache_dir)\n",
    "        fp = os.path.join(\n",
    "            tensor_datasets_cache_dir,\n",
    "            self.__repr__()\n",
    "        )\n",
    "        return fp\n",
    "\n",
    "    @property\n",
    "    def filepath(self):\n",
    "        tensor_datasets_cache_dir = os.path.join(\n",
    "            self.cache_dir,\n",
    "            'datasets')\n",
    "        if not os.path.exists(tensor_datasets_cache_dir):\n",
    "            os.mkdir(tensor_datasets_cache_dir)\n",
    "        return os.path.join(\n",
    "            self.cache_dir,\n",
    "            'datasets',\n",
    "            self.__repr__()\n",
    "        )\n",
    "\n",
    "    def data_loaders(self, batch_size, split=(0.85, 0.10)):\n",
    "        \"\"\"\n",
    "        Returns three data loaders obtained by splitting\n",
    "        self.tensor_dataset according to split\n",
    "        :param batch_size:\n",
    "        :param split:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        assert sum(split) < 1\n",
    "\n",
    "        dataset = self.tensor_dataset\n",
    "        num_examples = len(dataset)\n",
    "        a, b = split\n",
    "        train_dataset = TensorDataset(*dataset[: int(a * num_examples)])\n",
    "        val_dataset = TensorDataset(*dataset[int(a * num_examples):\n",
    "                                             int((a + b) * num_examples)])\n",
    "        eval_dataset = TensorDataset(*dataset[int((a + b) * num_examples):])\n",
    "\n",
    "        train_dl = DataLoader(\n",
    "            train_dataset,\n",
    "            batch_size=batch_size,\n",
    "            shuffle=True,\n",
    "            num_workers=4,\n",
    "            pin_memory=True,\n",
    "            drop_last=True,\n",
    "        )\n",
    "\n",
    "        val_dl = DataLoader(\n",
    "            val_dataset,\n",
    "            batch_size=batch_size,\n",
    "            shuffle=False,\n",
    "            num_workers=0,\n",
    "            pin_memory=False,\n",
    "            drop_last=True,\n",
    "        )\n",
    "\n",
    "        eval_dl = DataLoader(\n",
    "            eval_dataset,\n",
    "            batch_size=batch_size,\n",
    "            shuffle=False,\n",
    "            num_workers=0,\n",
    "            pin_memory=False,\n",
    "            drop_last=True,\n",
    "        )\n",
    "        return train_dl, val_dl, eval_dl\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f2780dfe",
   "metadata": {
    "id": "f2780dfe"
   },
   "outputs": [],
   "source": [
    "#@title Chorale Dataset\n",
    "class ChoraleDataset(MusicDataset):\n",
    "    \"\"\"\n",
    "    Class for all chorale-like datasets\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 corpus_it_gen,\n",
    "                 name,\n",
    "                 voice_ids,\n",
    "                 metadatas=None,\n",
    "                 sequences_size=8,\n",
    "                 subdivision=4,\n",
    "                 cache_dir=None):\n",
    "        \"\"\"\n",
    "        :param corpus_it_gen: calling this function returns an iterator\n",
    "        over chorales (as music21 scores)\n",
    "        :param name: name of the dataset\n",
    "        :param voice_ids: list of voice_indexes to be used\n",
    "        :param metadatas: list[Metadata], the list of used metadatas\n",
    "        :param sequences_size: in beats\n",
    "        :param subdivision: number of sixteenth notes per beat\n",
    "        :param cache_dir: directory where tensor_dataset is stored\n",
    "        \"\"\"\n",
    "        super(ChoraleDataset, self).__init__(cache_dir=cache_dir)\n",
    "        self.voice_ids = voice_ids\n",
    "        # TODO WARNING voice_ids is never used!\n",
    "        self.num_voices = len(voice_ids)\n",
    "        self.name = name\n",
    "        self.sequences_size = sequences_size\n",
    "        self.index2note_dicts = None\n",
    "        self.note2index_dicts = None\n",
    "        self.corpus_it_gen = corpus_it_gen\n",
    "        self.voice_ranges = None  # in midi pitch\n",
    "        self.metadatas = metadatas\n",
    "        self.subdivision = subdivision\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f'ChoraleDataset(' \\\n",
    "               f'{self.voice_ids},' \\\n",
    "               f'{self.name},' \\\n",
    "               f'{[metadata.name for metadata in self.metadatas]},' \\\n",
    "               f'{self.sequences_size},' \\\n",
    "               f'{self.subdivision})'\n",
    "\n",
    "    def iterator_gen(self):\n",
    "        return (chorale\n",
    "                for chorale in self.corpus_it_gen()\n",
    "                if self.is_valid(chorale)\n",
    "                )\n",
    "\n",
    "    def make_tensor_dataset(self):\n",
    "        \"\"\"\n",
    "        Implementation of the make_tensor_dataset abstract base class\n",
    "        \"\"\"\n",
    "        # todo check on chorale with Chord\n",
    "        print('Making tensor dataset')\n",
    "        self.compute_index_dicts()\n",
    "        self.compute_voice_ranges()\n",
    "        one_tick = 1 / self.subdivision\n",
    "        chorale_tensor_dataset = []\n",
    "        metadata_tensor_dataset = []\n",
    "        for chorale_id, chorale in tqdm(enumerate(self.iterator_gen())):\n",
    "\n",
    "            # precompute all possible transpositions and corresponding metadatas\n",
    "            chorale_transpositions = {}\n",
    "            metadatas_transpositions = {}\n",
    "\n",
    "            # main loop\n",
    "            for offsetStart in np.arange(\n",
    "                    chorale.flat.lowestOffset -\n",
    "                    (self.sequences_size - one_tick),\n",
    "                    chorale.flat.highestOffset,\n",
    "                    one_tick):\n",
    "                offsetEnd = offsetStart + self.sequences_size\n",
    "                current_subseq_ranges = self.voice_range_in_subsequence(\n",
    "                    chorale,\n",
    "                    offsetStart=offsetStart,\n",
    "                    offsetEnd=offsetEnd)\n",
    "\n",
    "                transposition = self.min_max_transposition(current_subseq_ranges)\n",
    "                min_transposition_subsequence, max_transposition_subsequence = transposition\n",
    "\n",
    "                for semi_tone in range(min_transposition_subsequence,\n",
    "                                       max_transposition_subsequence + 1):\n",
    "                    start_tick = int(offsetStart * self.subdivision)\n",
    "                    end_tick = int(offsetEnd * self.subdivision)\n",
    "\n",
    "                    try:\n",
    "                        # compute transpositions lazily\n",
    "                        if semi_tone not in chorale_transpositions:\n",
    "                            (chorale_tensor,\n",
    "                             metadata_tensor) = self.transposed_score_and_metadata_tensors(\n",
    "                                chorale,\n",
    "                                semi_tone=semi_tone)\n",
    "                            chorale_transpositions.update(\n",
    "                                {semi_tone:\n",
    "                                     chorale_tensor})\n",
    "                            metadatas_transpositions.update(\n",
    "                                {semi_tone:\n",
    "                                     metadata_tensor})\n",
    "                        else:\n",
    "                            chorale_tensor = chorale_transpositions[semi_tone]\n",
    "                            metadata_tensor = metadatas_transpositions[semi_tone]\n",
    "\n",
    "                        local_chorale_tensor = self.extract_score_tensor_with_padding(\n",
    "                            chorale_tensor,\n",
    "                            start_tick, end_tick)\n",
    "                        local_metadata_tensor = self.extract_metadata_with_padding(\n",
    "                            metadata_tensor,\n",
    "                            start_tick, end_tick)\n",
    "\n",
    "                        # append and add batch dimension\n",
    "                        # cast to int\n",
    "                        chorale_tensor_dataset.append(\n",
    "                            local_chorale_tensor[None, :, :].int())\n",
    "                        metadata_tensor_dataset.append(\n",
    "                            local_metadata_tensor[None, :, :, :].int())\n",
    "                    except KeyError:\n",
    "                        # some problems may occur with the key analyzer\n",
    "                        print(f'KeyError with chorale {chorale_id}')\n",
    "\n",
    "        chorale_tensor_dataset = torch.cat(chorale_tensor_dataset, 0)\n",
    "        metadata_tensor_dataset = torch.cat(metadata_tensor_dataset, 0)\n",
    "\n",
    "        dataset = TensorDataset(chorale_tensor_dataset,\n",
    "                                metadata_tensor_dataset)\n",
    "\n",
    "        print(f'Sizes: {chorale_tensor_dataset.size()}, {metadata_tensor_dataset.size()}')\n",
    "        return dataset\n",
    "\n",
    "    def transposed_score_and_metadata_tensors(self, score, semi_tone):\n",
    "        \"\"\"\n",
    "        Convert chorale to a couple (chorale_tensor, metadata_tensor),\n",
    "        the original chorale is transposed semi_tone number of semi-tones\n",
    "        :param chorale: music21 object\n",
    "        :param semi_tone:\n",
    "        :return: couple of tensors\n",
    "        \"\"\"\n",
    "        # transpose\n",
    "        # compute the most \"natural\" interval given a number of semi-tones\n",
    "        interval_type, interval_nature = interval.convertSemitoneToSpecifierGeneric(\n",
    "            semi_tone)\n",
    "        transposition_interval = interval.Interval(\n",
    "            str(interval_nature) + str(interval_type))\n",
    "\n",
    "        chorale_tranposed = score.transpose(transposition_interval)\n",
    "        chorale_tensor = self.get_score_tensor(\n",
    "            chorale_tranposed,\n",
    "            offsetStart=0.,\n",
    "            offsetEnd=chorale_tranposed.flat.highestTime)\n",
    "        metadatas_transposed = self.get_metadata_tensor(chorale_tranposed)\n",
    "        return chorale_tensor, metadatas_transposed\n",
    "\n",
    "    def get_metadata_tensor(self, score):\n",
    "        \"\"\"\n",
    "        Adds also the index of the voices\n",
    "        :param score: music21 stream\n",
    "        :return:tensor (num_voices, chorale_length, len(self.metadatas) + 1)\n",
    "        \"\"\"\n",
    "        md = []\n",
    "        if self.metadatas:\n",
    "            for metadata in self.metadatas:\n",
    "                sequence_metadata = torch.from_numpy(\n",
    "                    metadata.evaluate(score, self.subdivision)).long().clone()\n",
    "                square_metadata = sequence_metadata.repeat(self.num_voices, 1)\n",
    "                md.append(\n",
    "                    square_metadata[:, :, None]\n",
    "                )\n",
    "        chorale_length = int(score.duration.quarterLength * self.subdivision)\n",
    "\n",
    "        # add voice indexes\n",
    "        voice_id_metada = torch.from_numpy(np.arange(self.num_voices)).long().clone()\n",
    "        square_metadata = torch.transpose(voice_id_metada.repeat(chorale_length, 1),\n",
    "                                          0, 1)\n",
    "        md.append(square_metadata[:, :, None])\n",
    "\n",
    "        all_metadata = torch.cat(md, 2)\n",
    "        return all_metadata\n",
    "\n",
    "    def set_fermatas(self, metadata_tensor, fermata_tensor):\n",
    "        \"\"\"\n",
    "        Impose fermatas for all chorales in a batch\n",
    "        :param metadata_tensor: a (batch_size, sequences_size, num_metadatas)\n",
    "            tensor\n",
    "        :param fermata_tensor: a (sequences_size) binary tensor\n",
    "        \"\"\"\n",
    "        if self.metadatas:\n",
    "            for metadata_index, metadata in enumerate(self.metadatas):\n",
    "                if isinstance(metadata, FermataMetadata):\n",
    "                    # uses broadcasting\n",
    "                    metadata_tensor[:, :, metadata_index] = fermata_tensor\n",
    "                    break\n",
    "        return metadata_tensor\n",
    "\n",
    "    def add_fermata(self, metadata_tensor, time_index_start, time_index_stop):\n",
    "        \"\"\"\n",
    "        Shorthand function to impose a fermata between two time indexes\n",
    "        \"\"\"\n",
    "        fermata_tensor = torch.zeros(self.sequences_size)\n",
    "        fermata_tensor[time_index_start:time_index_stop] = 1\n",
    "        metadata_tensor = self.set_fermatas(metadata_tensor, fermata_tensor)\n",
    "        return metadata_tensor\n",
    "\n",
    "    def min_max_transposition(self, current_subseq_ranges):\n",
    "        if current_subseq_ranges is None:\n",
    "            # todo might be too restrictive\n",
    "            # there is no note in one part\n",
    "            transposition = (0, 0)  # min and max transpositions\n",
    "        else:\n",
    "            transpositions = [\n",
    "                (min_pitch_corpus - min_pitch_current,\n",
    "                 max_pitch_corpus - max_pitch_current)\n",
    "                for ((min_pitch_corpus, max_pitch_corpus),\n",
    "                     (min_pitch_current, max_pitch_current))\n",
    "                in zip(self.voice_ranges, current_subseq_ranges)\n",
    "            ]\n",
    "            transpositions = [min_or_max_transposition\n",
    "                              for min_or_max_transposition in zip(*transpositions)]\n",
    "            transposition = [max(transpositions[0]),\n",
    "                             min(transpositions[1])]\n",
    "        return transposition\n",
    "\n",
    "    def get_score_tensor(self, score, offsetStart, offsetEnd):\n",
    "        chorale_tensor = []\n",
    "        for part_id, part in enumerate(score.parts[:self.num_voices]):\n",
    "            part_tensor = self.part_to_tensor(part, part_id,\n",
    "                                              offsetStart=offsetStart,\n",
    "                                              offsetEnd=offsetEnd)\n",
    "            chorale_tensor.append(part_tensor)\n",
    "        return torch.cat(chorale_tensor, 0)\n",
    "\n",
    "    def part_to_tensor(self, part, part_id, offsetStart, offsetEnd):\n",
    "        \"\"\"\n",
    "        :param part:\n",
    "        :param part_id:\n",
    "        :param offsetStart:\n",
    "        :param offsetEnd:\n",
    "        :return: torch IntTensor (1, length)\n",
    "        \"\"\"\n",
    "        list_notes_and_rests = list(part.flat.getElementsByOffset(\n",
    "            offsetStart=offsetStart,\n",
    "            offsetEnd=offsetEnd,\n",
    "            classList=[music21.note.Note,\n",
    "                       music21.note.Rest]))\n",
    "        list_note_strings_and_pitches = [(n.nameWithOctave, n.pitch.midi)\n",
    "                                         for n in list_notes_and_rests\n",
    "                                         if n.isNote]\n",
    "        length = int((offsetEnd - offsetStart) * self.subdivision)  # in ticks\n",
    "\n",
    "        # add entries to dictionaries if not present\n",
    "        # should only be called by make_dataset when transposing\n",
    "        note2index = self.note2index_dicts[part_id]\n",
    "        index2note = self.index2note_dicts[part_id]\n",
    "        voice_range = self.voice_ranges[part_id]\n",
    "        min_pitch, max_pitch = voice_range\n",
    "        for note_name, pitch in list_note_strings_and_pitches:\n",
    "            # if out of range\n",
    "            if pitch < min_pitch or pitch > max_pitch:\n",
    "                note_name = OUT_OF_RANGE\n",
    "\n",
    "            if note_name not in note2index:\n",
    "                new_index = len(note2index)\n",
    "                index2note.update({new_index: note_name})\n",
    "                note2index.update({note_name: new_index})\n",
    "                print('Warning: Entry ' + str(\n",
    "                    {new_index: note_name}) + ' added to dictionaries')\n",
    "\n",
    "        # construct sequence\n",
    "        j = 0\n",
    "        i = 0\n",
    "        t = np.zeros((length, 2))\n",
    "        is_articulated = True\n",
    "        num_notes = len(list_notes_and_rests)\n",
    "        while i < length:\n",
    "            if j < num_notes - 1:\n",
    "                if (list_notes_and_rests[j + 1].offset > i\n",
    "                        / self.subdivision + offsetStart):\n",
    "                    t[i, :] = [note2index[standard_name(list_notes_and_rests[j],\n",
    "                                                        voice_range=voice_range)],\n",
    "                               is_articulated]\n",
    "                    i += 1\n",
    "                    is_articulated = False\n",
    "                else:\n",
    "                    j += 1\n",
    "                    is_articulated = True\n",
    "            else:\n",
    "                t[i, :] = [note2index[standard_name(list_notes_and_rests[j],\n",
    "                                                    voice_range=voice_range)],\n",
    "                           is_articulated]\n",
    "                i += 1\n",
    "                is_articulated = False\n",
    "        seq = t[:, 0] * t[:, 1] + (1 - t[:, 1]) * note2index[SLUR_SYMBOL]\n",
    "        tensor = torch.from_numpy(seq).long()[None, :]\n",
    "        return tensor\n",
    "\n",
    "    def voice_range_in_subsequence(self, chorale, offsetStart, offsetEnd):\n",
    "        \"\"\"\n",
    "        returns None if no note present in one of the voices -> no transposition\n",
    "        :param chorale:\n",
    "        :param offsetStart:\n",
    "        :param offsetEnd:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        voice_ranges = []\n",
    "        for part in chorale.parts[:self.num_voices]:\n",
    "            voice_range_part = self.voice_range_in_part(part,\n",
    "                                                        offsetStart=offsetStart,\n",
    "                                                        offsetEnd=offsetEnd)\n",
    "            if voice_range_part is None:\n",
    "                return None\n",
    "            else:\n",
    "                voice_ranges.append(voice_range_part)\n",
    "        return voice_ranges\n",
    "\n",
    "    def voice_range_in_part(self, part, offsetStart, offsetEnd):\n",
    "        notes_in_subsequence = part.flat.getElementsByOffset(\n",
    "            offsetStart,\n",
    "            offsetEnd,\n",
    "            includeEndBoundary=False,\n",
    "            mustBeginInSpan=True,\n",
    "            mustFinishInSpan=False,\n",
    "            classList=[music21.note.Note,\n",
    "                       music21.note.Rest])\n",
    "        midi_pitches_part = [\n",
    "            n.pitch.midi\n",
    "            for n in notes_in_subsequence\n",
    "            if n.isNote\n",
    "        ]\n",
    "        if len(midi_pitches_part) > 0:\n",
    "            return min(midi_pitches_part), max(midi_pitches_part)\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "    def compute_index_dicts(self):\n",
    "        print('Computing index dicts')\n",
    "        self.index2note_dicts = [\n",
    "            {} for _ in range(self.num_voices)\n",
    "        ]\n",
    "        self.note2index_dicts = [\n",
    "            {} for _ in range(self.num_voices)\n",
    "        ]\n",
    "\n",
    "        # create and add additional symbols\n",
    "        note_sets = [set() for _ in range(self.num_voices)]\n",
    "        for note_set in note_sets:\n",
    "            note_set.add(SLUR_SYMBOL)\n",
    "            note_set.add(START_SYMBOL)\n",
    "            note_set.add(END_SYMBOL)\n",
    "            note_set.add(REST_SYMBOL)\n",
    "\n",
    "        # get all notes: used for computing pitch ranges\n",
    "        for chorale in tqdm(self.iterator_gen()):\n",
    "            for part_id, part in enumerate(chorale.parts[:self.num_voices]):\n",
    "                for n in part.flat.notesAndRests:\n",
    "                    note_sets[part_id].add(standard_name(n))\n",
    "\n",
    "        # create tables\n",
    "        for note_set, index2note, note2index in zip(note_sets,\n",
    "                                                    self.index2note_dicts,\n",
    "                                                    self.note2index_dicts):\n",
    "            for note_index, note in enumerate(note_set):\n",
    "                index2note.update({note_index: note})\n",
    "                note2index.update({note: note_index})\n",
    "\n",
    "    def is_valid(self, chorale):\n",
    "        # We only consider 4-part chorales\n",
    "        if not len(chorale.parts) == 4:\n",
    "            return False\n",
    "        # todo contains chord\n",
    "        return True\n",
    "\n",
    "    def compute_voice_ranges(self):\n",
    "        assert self.index2note_dicts is not None\n",
    "        assert self.note2index_dicts is not None\n",
    "        self.voice_ranges = []\n",
    "        print('Computing voice ranges')\n",
    "        for voice_index, note2index in tqdm(enumerate(self.note2index_dicts)):\n",
    "            notes = [\n",
    "                standard_note(note_string)\n",
    "                for note_string in note2index\n",
    "            ]\n",
    "            midi_pitches = [\n",
    "                n.pitch.midi\n",
    "                for n in notes\n",
    "                if n.isNote\n",
    "            ]\n",
    "            min_midi, max_midi = min(midi_pitches), max(midi_pitches)\n",
    "            self.voice_ranges.append((min_midi, max_midi))\n",
    "\n",
    "    def extract_score_tensor_with_padding(self, tensor_score, start_tick, end_tick):\n",
    "        \"\"\"\n",
    "        :param tensor_chorale: (num_voices, length in ticks)\n",
    "        :param start_tick:\n",
    "        :param end_tick:\n",
    "        :return: tensor_chorale[:, start_tick: end_tick]\n",
    "        with padding if necessary\n",
    "        i.e. if start_tick < 0 or end_tick > tensor_chorale length\n",
    "        \"\"\"\n",
    "        assert start_tick < end_tick\n",
    "        assert end_tick > 0\n",
    "        length = tensor_score.size()[1]\n",
    "\n",
    "        padded_chorale = []\n",
    "        # todo add PAD_SYMBOL\n",
    "        if start_tick < 0:\n",
    "            start_symbols = np.array([note2index[START_SYMBOL]\n",
    "                                      for note2index in self.note2index_dicts])\n",
    "            start_symbols = torch.from_numpy(start_symbols).long().clone()\n",
    "            start_symbols = start_symbols.repeat(-start_tick, 1).transpose(0, 1)\n",
    "            padded_chorale.append(start_symbols)\n",
    "\n",
    "        slice_start = start_tick if start_tick > 0 else 0\n",
    "        slice_end = end_tick if end_tick < length else length\n",
    "\n",
    "        padded_chorale.append(tensor_score[:, slice_start: slice_end])\n",
    "\n",
    "        if end_tick > length:\n",
    "            end_symbols = np.array([note2index[END_SYMBOL]\n",
    "                                    for note2index in self.note2index_dicts])\n",
    "            end_symbols = torch.from_numpy(end_symbols).long().clone()\n",
    "            end_symbols = end_symbols.repeat(end_tick - length, 1).transpose(0, 1)\n",
    "            padded_chorale.append(end_symbols)\n",
    "\n",
    "        padded_chorale = torch.cat(padded_chorale, 1)\n",
    "        return padded_chorale\n",
    "\n",
    "    def extract_metadata_with_padding(self, tensor_metadata,\n",
    "                                      start_tick, end_tick):\n",
    "        \"\"\"\n",
    "        :param tensor_metadata: (num_voices, length, num_metadatas)\n",
    "        last metadata is the voice_index\n",
    "        :param start_tick:\n",
    "        :param end_tick:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        assert start_tick < end_tick\n",
    "        assert end_tick > 0\n",
    "        num_voices, length, num_metadatas = tensor_metadata.size()\n",
    "        padded_tensor_metadata = []\n",
    "\n",
    "        if start_tick < 0:\n",
    "            # TODO more subtle padding\n",
    "            start_symbols = np.zeros((self.num_voices, -start_tick, num_metadatas))\n",
    "            start_symbols = torch.from_numpy(start_symbols).long().clone()\n",
    "            padded_tensor_metadata.append(start_symbols)\n",
    "\n",
    "        slice_start = start_tick if start_tick > 0 else 0\n",
    "        slice_end = end_tick if end_tick < length else length\n",
    "        padded_tensor_metadata.append(tensor_metadata[:, slice_start: slice_end, :])\n",
    "\n",
    "        if end_tick > length:\n",
    "            end_symbols = np.zeros((self.num_voices, end_tick - length, num_metadatas))\n",
    "            end_symbols = torch.from_numpy(end_symbols).long().clone()\n",
    "            padded_tensor_metadata.append(end_symbols)\n",
    "\n",
    "        padded_tensor_metadata = torch.cat(padded_tensor_metadata, 1)\n",
    "        return padded_tensor_metadata\n",
    "\n",
    "    def empty_score_tensor(self, score_length):\n",
    "        start_symbols = np.array([note2index[START_SYMBOL]\n",
    "                                  for note2index in self.note2index_dicts])\n",
    "        start_symbols = torch.from_numpy(start_symbols).long().clone()\n",
    "        start_symbols = start_symbols.repeat(score_length, 1).transpose(0, 1)\n",
    "        return start_symbols\n",
    "\n",
    "    def random_score_tensor(self, score_length):\n",
    "        chorale_tensor = np.array(\n",
    "            [np.random.randint(len(note2index),\n",
    "                               size=score_length)\n",
    "             for note2index in self.note2index_dicts])\n",
    "        chorale_tensor = torch.from_numpy(chorale_tensor).long().clone()\n",
    "        return chorale_tensor\n",
    "\n",
    "    def tensor_to_score(self, tensor_score,\n",
    "            fermata_tensor=None):\n",
    "        \"\"\"\n",
    "        :param tensor_score: (num_voices, length)\n",
    "        :return: music21 score object\n",
    "        \"\"\"\n",
    "        slur_indexes = [note2index[SLUR_SYMBOL]\n",
    "                        for note2index in self.note2index_dicts]\n",
    "\n",
    "        score = music21.stream.Score()\n",
    "        num_voices = tensor_score.size(0)\n",
    "        name_parts = (num_voices == 4)\n",
    "        part_names = ['Soprano', 'Alto', 'Tenor', 'Bass']\n",
    "\n",
    "        \n",
    "        for voice_index, (voice, index2note, slur_index) in enumerate(\n",
    "                zip(tensor_score,\n",
    "                    self.index2note_dicts,\n",
    "                    slur_indexes)):\n",
    "            add_fermata = False\n",
    "            if name_parts:\n",
    "                part = stream.Part(id=part_names[voice_index],\n",
    "                partName=part_names[voice_index],\n",
    "                partAbbreviation=part_names[voice_index],\n",
    "                instrumentName=part_names[voice_index])\n",
    "            else:\n",
    "                part = stream.Part(id='part' + str(voice_index))\n",
    "            dur = 0\n",
    "            total_duration = 0\n",
    "            f = music21.note.Rest()\n",
    "            for note_index in [n.item() for n in voice]:\n",
    "                # if it is a played note\n",
    "                if not note_index == slur_indexes[voice_index]:\n",
    "                    # add previous note\n",
    "                    if dur > 0:\n",
    "                        f.duration = music21.duration.Duration(dur / self.subdivision)\n",
    "\n",
    "                        if add_fermata:\n",
    "                            f.expressions.append(music21.expressions.Fermata())\n",
    "                            add_fermata = False\n",
    "\n",
    "                        part.append(f)\n",
    "\n",
    "\n",
    "                    dur = 1\n",
    "                    f = standard_note(index2note[note_index])\n",
    "                    if fermata_tensor is not None and voice_index == 0:\n",
    "                        if fermata_tensor[0, total_duration] == 1:\n",
    "                            add_fermata = True\n",
    "                        else:\n",
    "                            add_fermata = False\n",
    "                    total_duration += 1\n",
    "\n",
    "                else:\n",
    "                    dur += 1\n",
    "                    total_duration += 1\n",
    "            # add last note\n",
    "            f.duration = music21.duration.Duration(dur / self.subdivision)\n",
    "            if add_fermata:\n",
    "                f.expressions.append(music21.expressions.Fermata())\n",
    "                add_fermata = False\n",
    "\n",
    "            part.append(f)\n",
    "            score.insert(part)\n",
    "        return score\n",
    "\n",
    "\n",
    "# TODO should go in ChoraleDataset\n",
    "# TODO all subsequences start on a beat\n",
    "class ChoraleBeatsDataset(ChoraleDataset):\n",
    "    def __repr__(self):\n",
    "        return f'ChoraleBeatsDataset(' \\\n",
    "               f'{self.voice_ids},' \\\n",
    "               f'{self.name},' \\\n",
    "               f'{[metadata.name for metadata in self.metadatas]},' \\\n",
    "               f'{self.sequences_size},' \\\n",
    "               f'{self.subdivision})'\n",
    "\n",
    "    def make_tensor_dataset(self):\n",
    "        \"\"\"\n",
    "        Implementation of the make_tensor_dataset abstract base class\n",
    "        \"\"\"\n",
    "        # todo check on chorale with Chord\n",
    "        print('Making tensor dataset')\n",
    "        self.compute_index_dicts()\n",
    "        self.compute_voice_ranges()\n",
    "        one_beat = 1.\n",
    "        chorale_tensor_dataset = []\n",
    "        metadata_tensor_dataset = []\n",
    "        for chorale_id, chorale in tqdm(enumerate(self.iterator_gen())):\n",
    "\n",
    "            # precompute all possible transpositions and corresponding metadatas\n",
    "            chorale_transpositions = {}\n",
    "            metadatas_transpositions = {}\n",
    "\n",
    "            # main loop\n",
    "            for offsetStart in np.arange(\n",
    "                    chorale.flat.lowestOffset -\n",
    "                    (self.sequences_size - one_beat),\n",
    "                    chorale.flat.highestOffset,\n",
    "                    one_beat):\n",
    "                offsetEnd = offsetStart + self.sequences_size\n",
    "                current_subseq_ranges = self.voice_range_in_subsequence(\n",
    "                    chorale,\n",
    "                    offsetStart=offsetStart,\n",
    "                    offsetEnd=offsetEnd)\n",
    "\n",
    "                transposition = self.min_max_transposition(current_subseq_ranges)\n",
    "                min_transposition_subsequence, max_transposition_subsequence = transposition\n",
    "\n",
    "                for semi_tone in range(min_transposition_subsequence,\n",
    "                                       max_transposition_subsequence + 1):\n",
    "                    start_tick = int(offsetStart * self.subdivision)\n",
    "                    end_tick = int(offsetEnd * self.subdivision)\n",
    "\n",
    "                    try:\n",
    "                        # compute transpositions lazily\n",
    "                        if semi_tone not in chorale_transpositions:\n",
    "                            (chorale_tensor,\n",
    "                             metadata_tensor) = self.transposed_score_and_metadata_tensors(\n",
    "                                chorale,\n",
    "                                semi_tone=semi_tone)\n",
    "                            chorale_transpositions.update(\n",
    "                                {semi_tone:\n",
    "                                     chorale_tensor})\n",
    "                            metadatas_transpositions.update(\n",
    "                                {semi_tone:\n",
    "                                     metadata_tensor})\n",
    "                        else:\n",
    "                            chorale_tensor = chorale_transpositions[semi_tone]\n",
    "                            metadata_tensor = metadatas_transpositions[semi_tone]\n",
    "\n",
    "                        local_chorale_tensor = self.extract_score_tensor_with_padding(\n",
    "                            chorale_tensor,\n",
    "                            start_tick, end_tick)\n",
    "                        local_metadata_tensor = self.extract_metadata_with_padding(\n",
    "                            metadata_tensor,\n",
    "                            start_tick, end_tick)\n",
    "\n",
    "                        # append and add batch dimension\n",
    "                        # cast to int\n",
    "                        chorale_tensor_dataset.append(\n",
    "                            local_chorale_tensor[None, :, :].int())\n",
    "                        metadata_tensor_dataset.append(\n",
    "                            local_metadata_tensor[None, :, :, :].int())\n",
    "                    except KeyError:\n",
    "                        # some problems may occur with the key analyzer\n",
    "                        print(f'KeyError with chorale {chorale_id}')\n",
    "\n",
    "        chorale_tensor_dataset = torch.cat(chorale_tensor_dataset, 0)\n",
    "        metadata_tensor_dataset = torch.cat(metadata_tensor_dataset, 0)\n",
    "\n",
    "        dataset = TensorDataset(chorale_tensor_dataset,\n",
    "                                metadata_tensor_dataset)\n",
    "\n",
    "        print(f'Sizes: {chorale_tensor_dataset.size()}, {metadata_tensor_dataset.size()}')\n",
    "        return dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a80c10fe",
   "metadata": {
    "id": "a80c10fe"
   },
   "outputs": [],
   "source": [
    "#@title Dataset Manager\n",
    "# Basically, all you have to do to use an existing dataset is to\n",
    "# add an entry in the all_datasets variable\n",
    "# and specify its base class and which music21 objects it uses\n",
    "# by giving an iterator over music21 scores\n",
    "\n",
    "all_datasets = {\n",
    "    'bach_chorales':\n",
    "        {\n",
    "            'dataset_class_name': ChoraleDataset,\n",
    "            'corpus_it_gen':      music21.corpus.chorales.Iterator\n",
    "        },\n",
    "    'bach_chorales_test':\n",
    "        {\n",
    "            'dataset_class_name': ChoraleDataset,\n",
    "            'corpus_it_gen':      ShortChoraleIteratorGen()\n",
    "        },\n",
    "}\n",
    "\n",
    "\n",
    "class DatasetManager:\n",
    "    def __init__(self):\n",
    "        self.cache_dir = DATASET_CACHE_DIR\n",
    "        # create cache dir if it doesn't exist\n",
    "        if not os.path.exists(self.cache_dir):\n",
    "            os.mkdir(self.cache_dir)\n",
    "\n",
    "    def get_dataset(self, name: str, **dataset_kwargs) -> MusicDataset:\n",
    "        if name in all_datasets:\n",
    "            return self.load_if_exists_or_initialize_and_save(\n",
    "                name=name,\n",
    "                **all_datasets[name],\n",
    "                **dataset_kwargs\n",
    "            )\n",
    "        else:\n",
    "            print('Dataset with name {name} is not registered in all_datasets variable')\n",
    "            raise ValueError\n",
    "\n",
    "    def load_if_exists_or_initialize_and_save(self,\n",
    "                                              dataset_class_name,\n",
    "                                              corpus_it_gen,\n",
    "                                              name,\n",
    "                                              **kwargs):\n",
    "        \"\"\"\n",
    "\n",
    "        :param dataset_class_name:\n",
    "        :param corpus_it_gen:\n",
    "        :param name:\n",
    "        :param kwargs: parameters specific to an implementation\n",
    "        of MusicDataset (ChoraleDataset for instance)\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        kwargs.update(\n",
    "            {'name':          name,\n",
    "             'corpus_it_gen': corpus_it_gen,\n",
    "             'cache_dir':     self.cache_dir\n",
    "             })\n",
    "        dataset = dataset_class_name(**kwargs)\n",
    "        if os.path.exists(dataset.filepath):\n",
    "            print(f'Loading {dataset.__repr__()} from {dataset.filepath}')\n",
    "            dataset = torch.load(dataset.filepath)\n",
    "            dataset.cache_dir = self.cache_dir\n",
    "            print(f'(the corresponding TensorDataset is not loaded)')\n",
    "        else:\n",
    "            print(f'Creating {dataset.__repr__()}, '\n",
    "                  f'both tensor dataset and parameters')\n",
    "            # initialize and force the computation of the tensor_dataset\n",
    "            # first remove the cached data if it exists\n",
    "            if os.path.exists(dataset.tensor_dataset_filepath):\n",
    "                os.remove(dataset.tensor_dataset_filepath)\n",
    "            # recompute dataset parameters and tensor_dataset\n",
    "            # this saves the tensor_dataset in dataset.tensor_dataset_filepath\n",
    "            tensor_dataset = dataset.tensor_dataset\n",
    "            # save all dataset parameters EXCEPT the tensor dataset\n",
    "            # which is stored elsewhere\n",
    "            dataset.tensor_dataset = None\n",
    "            torch.save(dataset, dataset.filepath)\n",
    "            print(f'{dataset.__repr__()} saved in {dataset.filepath}')\n",
    "            dataset.tensor_dataset = tensor_dataset\n",
    "        return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a010e5cb",
   "metadata": {
    "id": "a010e5cb"
   },
   "outputs": [],
   "source": [
    "#@title Dataset Manager Usage example\n",
    "run_example = False #@param {type:\"boolean\"}\n",
    "# Usage example\n",
    "if run_example :\n",
    "    dataset_manager = DatasetManager()\n",
    "    subdivision = 4\n",
    "    metadatas = [\n",
    "        TickMetadata(subdivision=subdivision),\n",
    "        FermataMetadata(),\n",
    "        KeyMetadata()\n",
    "    ]\n",
    "\n",
    "    bach_chorales_dataset: ChoraleDataset = dataset_manager.get_dataset(\n",
    "        name='bach_chorales_test',\n",
    "        voice_ids=[0, 1, 2, 3],\n",
    "        metadatas=metadatas,\n",
    "        sequences_size=8,\n",
    "        subdivision=subdivision\n",
    "    )\n",
    "    (train_dataloader,\n",
    "     val_dataloader,\n",
    "     test_dataloader) = bach_chorales_dataset.data_loaders(\n",
    "        batch_size=128,\n",
    "        split=(0.85, 0.10)\n",
    "    )\n",
    "    print('Num Train Batches: ', len(train_dataloader))\n",
    "    print('Num Valid Batches: ', len(val_dataloader))\n",
    "    print('Num Test Batches: ', len(test_dataloader))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Cx_IMqN_UJfz",
   "metadata": {
    "id": "Cx_IMqN_UJfz"
   },
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f8105409",
   "metadata": {
    "cellView": "form",
    "id": "f8105409"
   },
   "outputs": [],
   "source": [
    "#@title Voice Model\n",
    "class VoiceModel(nn.Module):\n",
    "    def __init__(self,\n",
    "                 dataset: ChoraleDataset,\n",
    "                 main_voice_index: int,\n",
    "                 note_embedding_dim: int,\n",
    "                 meta_embedding_dim: int,\n",
    "                 num_layers: int,\n",
    "                 lstm_hidden_size: int,\n",
    "                 dropout_lstm: float,\n",
    "                 hidden_size_linear=200\n",
    "                 ):\n",
    "        super(VoiceModel, self).__init__()\n",
    "        self.dataset = dataset\n",
    "        self.main_voice_index = main_voice_index\n",
    "        self.note_embedding_dim = note_embedding_dim\n",
    "        self.meta_embedding_dim = meta_embedding_dim\n",
    "        self.num_notes_per_voice = [len(d)\n",
    "                                    for d in dataset.note2index_dicts]\n",
    "        self.num_voices = self.dataset.num_voices\n",
    "        self.num_metas_per_voice = [\n",
    "                                       metadata.num_values\n",
    "                                       for metadata in dataset.metadatas\n",
    "                                   ] + [self.num_voices]\n",
    "        self.num_metas = len(self.dataset.metadatas) + 1\n",
    "        self.num_layers = num_layers\n",
    "        self.lstm_hidden_size = lstm_hidden_size\n",
    "        self.dropout_lstm = dropout_lstm\n",
    "        self.hidden_size_linear = hidden_size_linear\n",
    "\n",
    "        self.other_voices_indexes = [i\n",
    "                                     for i\n",
    "                                     in range(self.num_voices)\n",
    "                                     if not i == main_voice_index]\n",
    "\n",
    "        self.note_embeddings = nn.ModuleList(\n",
    "            [nn.Embedding(num_notes, note_embedding_dim)\n",
    "             for num_notes in self.num_notes_per_voice]\n",
    "        )\n",
    "        self.meta_embeddings = nn.ModuleList(\n",
    "            [nn.Embedding(num_metas, meta_embedding_dim)\n",
    "             for num_metas in self.num_metas_per_voice]\n",
    "        )\n",
    "\n",
    "        self.lstm_left = nn.LSTM(input_size=note_embedding_dim * self.num_voices +\n",
    "                                            meta_embedding_dim * self.num_metas,\n",
    "                                 hidden_size=lstm_hidden_size,\n",
    "                                 num_layers=num_layers,\n",
    "                                 dropout=dropout_lstm,\n",
    "                                 batch_first=True)\n",
    "        self.lstm_right = nn.LSTM(input_size=note_embedding_dim * self.num_voices +\n",
    "                                             meta_embedding_dim * self.num_metas,\n",
    "                                  hidden_size=lstm_hidden_size,\n",
    "                                  num_layers=num_layers,\n",
    "                                  dropout=dropout_lstm,\n",
    "                                  batch_first=True)\n",
    "\n",
    "        self.mlp_center = nn.Sequential(\n",
    "            nn.Linear((note_embedding_dim * (self.num_voices - 1)\n",
    "                       + meta_embedding_dim * self.num_metas),\n",
    "                      hidden_size_linear),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size_linear, lstm_hidden_size)\n",
    "        )\n",
    "\n",
    "        self.mlp_predictions = nn.Sequential(\n",
    "            nn.Linear(self.lstm_hidden_size * 3,\n",
    "                      hidden_size_linear),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size_linear, self.num_notes_per_voice[main_voice_index])\n",
    "        )\n",
    "\n",
    "    def forward(self, *input):\n",
    "        notes, metas = input\n",
    "        batch_size, num_voices, timesteps_ticks = notes[0].size()\n",
    "\n",
    "        # put time first\n",
    "        ln, cn, rn = notes\n",
    "        ln, rn = [t.transpose(1, 2)\n",
    "                  for t in (ln, rn)]\n",
    "        notes = ln, cn, rn\n",
    "\n",
    "        # embedding\n",
    "        notes_embedded = self.embed(notes, type='note')\n",
    "        metas_embedded = self.embed(metas, type='meta')\n",
    "        # lists of (N, timesteps_ticks, voices * dim_embedding)\n",
    "        # where timesteps_ticks is 1 for central parts\n",
    "\n",
    "        # concat notes and metas\n",
    "        input_embedded = [torch.cat([notes, metas], 2) if notes is not None else None\n",
    "                          for notes, metas in zip(notes_embedded, metas_embedded)]\n",
    "\n",
    "        left, center, right = input_embedded\n",
    "\n",
    "        # main part\n",
    "        hidden = init_hidden(\n",
    "            num_layers=self.num_layers,\n",
    "            batch_size=batch_size,\n",
    "            lstm_hidden_size=self.lstm_hidden_size,\n",
    "        )\n",
    "        left, hidden = self.lstm_left(left, hidden)\n",
    "        left = left[:, -1, :]\n",
    "\n",
    "        if self.num_voices == 1:\n",
    "            center = cuda_variable(torch.zeros(\n",
    "                batch_size,\n",
    "                self.lstm_hidden_size)\n",
    "            )\n",
    "        else:\n",
    "            center = center[:, 0, :]  # remove time dimension\n",
    "            center = self.mlp_center(center)\n",
    "\n",
    "        hidden = init_hidden(\n",
    "            num_layers=self.num_layers,\n",
    "            batch_size=batch_size,\n",
    "            lstm_hidden_size=self.lstm_hidden_size,\n",
    "        )\n",
    "        right, hidden = self.lstm_right(right, hidden)\n",
    "        right = right[:, -1, :]\n",
    "\n",
    "        # concat and return prediction\n",
    "        predictions = torch.cat([\n",
    "            left, center, right\n",
    "        ], 1)\n",
    "\n",
    "        predictions = self.mlp_predictions(predictions)\n",
    "\n",
    "        return predictions\n",
    "\n",
    "    def embed(self, notes_or_metas, type):\n",
    "        if type == 'note':\n",
    "            embeddings = self.note_embeddings\n",
    "            embedding_dim = self.note_embedding_dim\n",
    "            other_voices_indexes = self.other_voices_indexes\n",
    "        elif type == 'meta':\n",
    "            embeddings = self.meta_embeddings\n",
    "            embedding_dim = self.meta_embedding_dim\n",
    "            other_voices_indexes = range(self.num_metas)\n",
    "\n",
    "        batch_size, timesteps_left_ticks, num_voices = notes_or_metas[0].size()\n",
    "        batch_size, timesteps_right_ticks, num_voices = notes_or_metas[2].size()\n",
    "\n",
    "        left, center, right = notes_or_metas\n",
    "        # center has self.num_voices - 1 voices\n",
    "        left_embedded = torch.cat([\n",
    "            embeddings[voice_id](left[:, :, voice_id])[:, :, None, :]\n",
    "            for voice_id in range(num_voices)\n",
    "        ], 2)\n",
    "        right_embedded = torch.cat([\n",
    "            embeddings[voice_id](right[:, :, voice_id])[:, :, None, :]\n",
    "            for voice_id in range(num_voices)\n",
    "        ], 2)\n",
    "        if self.num_voices == 1 and type == 'note':\n",
    "            center_embedded = None\n",
    "        else:\n",
    "            center_embedded = torch.cat([\n",
    "                embeddings[voice_id](center[:, k].unsqueeze(1))\n",
    "                for k, voice_id in enumerate(other_voices_indexes)\n",
    "            ], 1)\n",
    "            center_embedded = center_embedded.view(batch_size,\n",
    "                                                   1,\n",
    "                                                   len(other_voices_indexes) * embedding_dim)\n",
    "\n",
    "        # squeeze two last dimensions\n",
    "        left_embedded = left_embedded.view(batch_size,\n",
    "                                           timesteps_left_ticks,\n",
    "                                           num_voices * embedding_dim)\n",
    "        right_embedded = right_embedded.view(batch_size,\n",
    "                                             timesteps_right_ticks,\n",
    "                                             num_voices * embedding_dim)\n",
    "\n",
    "        return left_embedded, center_embedded, right_embedded\n",
    "\n",
    "    def save(self):\n",
    "        torch.save(self.state_dict(), MODELS_SAVE_DIR + self.__repr__())\n",
    "        print(f'Model {self.__repr__()} saved')\n",
    "\n",
    "    def load(self):\n",
    "        state_dict = torch.load(MODELS_SAVE_DIR + self.__repr__(),\n",
    "                                map_location=lambda storage, loc: storage)\n",
    "        print(f'Loading {self.__repr__()}')\n",
    "        self.load_state_dict(state_dict)\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f'VoiceModel(' \\\n",
    "               f'{self.dataset.__repr__()},' \\\n",
    "               f'{self.main_voice_index},' \\\n",
    "               f'{self.note_embedding_dim},' \\\n",
    "               f'{self.meta_embedding_dim},' \\\n",
    "               f'{self.num_layers},' \\\n",
    "               f'{self.lstm_hidden_size},' \\\n",
    "               f'{self.dropout_lstm},' \\\n",
    "               f'{self.hidden_size_linear}' \\\n",
    "               f')'\n",
    "\n",
    "    def train_model(self,\n",
    "                    batch_size=16,\n",
    "                    num_epochs=10,\n",
    "                    optimizer=None):\n",
    "        for epoch in range(num_epochs):\n",
    "            print(f'===Epoch {epoch}===')\n",
    "            (dataloader_train,\n",
    "             dataloader_val,\n",
    "             dataloader_test) = self.dataset.data_loaders(\n",
    "                batch_size=batch_size,\n",
    "            )\n",
    "\n",
    "            loss, acc = self.loss_and_acc(dataloader_train,\n",
    "                                          optimizer=optimizer,\n",
    "                                          phase='train')\n",
    "            print(f'Training loss: {loss}')\n",
    "            print(f'Training accuracy: {acc}')\n",
    "            # writer.add_scalar('data/training_loss', loss, epoch)\n",
    "            # writer.add_scalar('data/training_acc', acc, epoch)\n",
    "\n",
    "            loss, acc = self.loss_and_acc(dataloader_val,\n",
    "                                          optimizer=None,\n",
    "                                          phase='test')\n",
    "            print(f'Validation loss: {loss}')\n",
    "            print(f'Validation accuracy: {acc}')\n",
    "            self.save()\n",
    "\n",
    "    def loss_and_acc(self, dataloader,\n",
    "                     optimizer=None,\n",
    "                     phase='train'):\n",
    "\n",
    "        average_loss = 0\n",
    "        average_acc = 0\n",
    "        if phase == 'train':\n",
    "            self.train()\n",
    "        elif phase == 'eval' or phase == 'test':\n",
    "            self.eval()\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "\n",
    "        for tensor_chorale, tensor_metadata in tqdm(dataloader):\n",
    "            # to Variable\n",
    "            tensor_chorale = cuda_variable(tensor_chorale).long()\n",
    "            tensor_metadata = cuda_variable(tensor_metadata).long()\n",
    "\n",
    "            # preprocessing to put in the DeepBach format\n",
    "            # see Fig. 4 in DeepBach paper:\n",
    "            # https://arxiv.org/pdf/1612.01010.pdf\n",
    "            notes, metas, label = self.preprocess_input(tensor_chorale, tensor_metadata)\n",
    "            \n",
    "            weights = self.forward(notes, metas)\n",
    "            \n",
    "            loss_function = torch.nn.CrossEntropyLoss()\n",
    "            loss = loss_function(weights, label)\n",
    "\n",
    "            if phase == 'train':\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "            acc = self.accuracy(weights=weights, target=label)\n",
    "            \n",
    "            average_loss += loss.item()\n",
    "            average_acc += acc.item()\n",
    "\n",
    "        average_loss /= len(dataloader)\n",
    "        average_acc /= len(dataloader)\n",
    "        return average_loss, average_acc\n",
    "\n",
    "    def accuracy(self, weights, target):\n",
    "        batch_size, = target.size()\n",
    "        softmax = nn.Softmax(dim=1)(weights)\n",
    "        pred = softmax.max(1)[1].type_as(target)\n",
    "        num_corrects = (pred == target).float().sum()\n",
    "        return num_corrects / batch_size * 100\n",
    "\n",
    "    def preprocess_input(self, tensor_chorale, tensor_metadata):\n",
    "        \"\"\"\n",
    "        :param tensor_chorale: (batch_size, num_voices, chorale_length_ticks)\n",
    "        :param tensor_metadata: (batch_size, num_metadata, chorale_length_ticks)\n",
    "        :return: (notes, metas, label) tuple\n",
    "        where\n",
    "        notes = (left_notes, central_notes, right_notes)\n",
    "        metas = (left_metas, central_metas, right_metas)\n",
    "        label = (batch_size)\n",
    "        right_notes and right_metas are REVERSED (from right to left)\n",
    "        \"\"\"\n",
    "        batch_size, num_voices, chorale_length_ticks = tensor_chorale.size()\n",
    "\n",
    "        # random shift! Depends on the dataset\n",
    "        offset = random.randint(0, self.dataset.subdivision)\n",
    "        time_index_ticks = chorale_length_ticks // 2 + offset\n",
    "\n",
    "        # split notes\n",
    "        notes, label = self.preprocess_notes(tensor_chorale, time_index_ticks)\n",
    "        metas = self.preprocess_metas(tensor_metadata, time_index_ticks)\n",
    "        return notes, metas, label\n",
    "\n",
    "    def preprocess_notes(self, tensor_chorale, time_index_ticks):\n",
    "        \"\"\"\n",
    "\n",
    "        :param tensor_chorale: (batch_size, num_voices, chorale_length_ticks)\n",
    "        :param time_index_ticks:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        batch_size, num_voices, _ = tensor_chorale.size()\n",
    "        left_notes = tensor_chorale[:, :, :time_index_ticks]\n",
    "        right_notes = reverse_tensor(\n",
    "            tensor_chorale[:, :, time_index_ticks + 1:],\n",
    "            dim=2)\n",
    "        if self.num_voices == 1:\n",
    "            central_notes = None\n",
    "        else:\n",
    "            central_notes = mask_entry(tensor_chorale[:, :, time_index_ticks],\n",
    "                                       entry_index=self.main_voice_index,\n",
    "                                       dim=1)\n",
    "        label = tensor_chorale[:, self.main_voice_index, time_index_ticks]\n",
    "        return (left_notes, central_notes, right_notes), label\n",
    "\n",
    "    def preprocess_metas(self, tensor_metadata, time_index_ticks):\n",
    "        \"\"\"\n",
    "\n",
    "        :param tensor_metadata: (batch_size, num_voices, chorale_length_ticks)\n",
    "        :param time_index_ticks:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "\n",
    "        left_metas = tensor_metadata[:, self.main_voice_index, :time_index_ticks, :]\n",
    "        right_metas = reverse_tensor(\n",
    "            tensor_metadata[:, self.main_voice_index, time_index_ticks + 1:, :],\n",
    "            dim=1)\n",
    "        central_metas = tensor_metadata[:, self.main_voice_index, time_index_ticks, :]\n",
    "        return left_metas, central_metas, right_metas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "31bf4f9a",
   "metadata": {
    "cellView": "form",
    "id": "31bf4f9a"
   },
   "outputs": [],
   "source": [
    "#@title DeepBach - Model Manager\n",
    "class DeepBach:\n",
    "    def __init__(self,\n",
    "                 dataset,\n",
    "                 note_embedding_dim,\n",
    "                 meta_embedding_dim,\n",
    "                 num_layers,\n",
    "                 lstm_hidden_size,\n",
    "                 dropout_lstm,\n",
    "                 linear_hidden_size,\n",
    "                 ):\n",
    "        self.dataset = dataset\n",
    "        self.num_voices = self.dataset.num_voices\n",
    "        self.num_metas = len(self.dataset.metadatas) + 1\n",
    "        self.activate_cuda = torch.cuda.is_available()\n",
    "\n",
    "        self.voice_models = [VoiceModel(\n",
    "            dataset=self.dataset,\n",
    "            main_voice_index=main_voice_index,\n",
    "            note_embedding_dim=note_embedding_dim,\n",
    "            meta_embedding_dim=meta_embedding_dim,\n",
    "            num_layers=num_layers,\n",
    "            lstm_hidden_size=lstm_hidden_size,\n",
    "            dropout_lstm=dropout_lstm,\n",
    "            hidden_size_linear=linear_hidden_size,\n",
    "        )\n",
    "            for main_voice_index in range(self.num_voices)\n",
    "        ]\n",
    "\n",
    "    def cuda(self, main_voice_index=None):\n",
    "        if self.activate_cuda:\n",
    "            if main_voice_index is None:\n",
    "                for voice_index in range(self.num_voices):\n",
    "                    self.cuda(voice_index)\n",
    "            else:\n",
    "                self.voice_models[main_voice_index].cuda()\n",
    "\n",
    "    # Utils\n",
    "    def load(self, main_voice_index=None):\n",
    "        if main_voice_index is None:\n",
    "            for voice_index in range(self.num_voices):\n",
    "                self.load(main_voice_index=voice_index)\n",
    "        else:\n",
    "            self.voice_models[main_voice_index].load()\n",
    "\n",
    "    def save(self, main_voice_index=None):\n",
    "        if main_voice_index is None:\n",
    "            for voice_index in range(self.num_voices):\n",
    "                self.save(main_voice_index=voice_index)\n",
    "        else:\n",
    "            self.voice_models[main_voice_index].save()\n",
    "\n",
    "    def train(self, main_voice_index=None,\n",
    "              **kwargs):\n",
    "        if main_voice_index is None:\n",
    "            for voice_index in range(self.num_voices):\n",
    "                self.train(main_voice_index=voice_index, **kwargs)\n",
    "        else:\n",
    "            voice_model = self.voice_models[main_voice_index]\n",
    "            if self.activate_cuda:\n",
    "                voice_model.cuda()\n",
    "            optimizer = optim.Adam(voice_model.parameters())\n",
    "            voice_model.train_model(optimizer=optimizer, **kwargs)\n",
    "\n",
    "    def eval_phase(self):\n",
    "        for voice_model in self.voice_models:\n",
    "            voice_model.eval()\n",
    "\n",
    "    def train_phase(self):\n",
    "        for voice_model in self.voice_models:\n",
    "            voice_model.train()\n",
    "\n",
    "    def generation(self,\n",
    "                   temperature=1.0,\n",
    "                   batch_size_per_voice=8,\n",
    "                   num_iterations=None,\n",
    "                   sequence_length_ticks=160,\n",
    "                   tensor_chorale=None,\n",
    "                   tensor_metadata=None,\n",
    "                   time_index_range_ticks=None,\n",
    "                   voice_index_range=None,\n",
    "                   fermatas=None,\n",
    "                   random_init=True\n",
    "                   ):\n",
    "        \"\"\"\n",
    "\n",
    "        :param temperature:\n",
    "        :param batch_size_per_voice:\n",
    "        :param num_iterations:\n",
    "        :param sequence_length_ticks:\n",
    "        :param tensor_chorale:\n",
    "        :param tensor_metadata:\n",
    "        :param time_index_range_ticks: list of two integers [a, b] or None; can be used \\\n",
    "        to regenerate only the portion of the score between timesteps a and b\n",
    "        :param voice_index_range: list of two integers [a, b] or None; can be used \\\n",
    "        to regenerate only the portion of the score between voice_index a and b\n",
    "        :param fermatas: list[Fermata]\n",
    "        :param random_init: boolean, whether or not to randomly initialize\n",
    "        the portion of the score on which we apply the pseudo-Gibbs algorithm\n",
    "        :return: tuple (\n",
    "        generated_score [music21 Stream object],\n",
    "        tensor_chorale (num_voices, chorale_length) torch.IntTensor,\n",
    "        tensor_metadata (num_voices, chorale_length, num_metadata) torch.IntTensor\n",
    "        )\n",
    "        \"\"\"\n",
    "        self.eval_phase()\n",
    "\n",
    "        # --Process arguments\n",
    "        # initialize generated chorale\n",
    "        # tensor_chorale = self.dataset.empty_chorale(sequence_length_ticks)\n",
    "        if tensor_chorale is None:\n",
    "            tensor_chorale = self.dataset.random_score_tensor(\n",
    "                sequence_length_ticks)\n",
    "        else:\n",
    "            sequence_length_ticks = tensor_chorale.size(1)\n",
    "\n",
    "        # initialize metadata\n",
    "        if tensor_metadata is None:\n",
    "            test_chorale = next(self.dataset.corpus_it_gen().__iter__())\n",
    "            tensor_metadata = self.dataset.get_metadata_tensor(test_chorale)\n",
    "\n",
    "            if tensor_metadata.size(1) < sequence_length_ticks:\n",
    "                tensor_metadata = tensor_metadata.repeat(1, sequence_length_ticks // tensor_metadata.size(1) + 1, 1)\n",
    "\n",
    "            # todo do not work if metadata_length_ticks > sequence_length_ticks\n",
    "            tensor_metadata = tensor_metadata[:, :sequence_length_ticks, :]\n",
    "        else:\n",
    "            tensor_metadata_length = tensor_metadata.size(1)\n",
    "            assert tensor_metadata_length == sequence_length_ticks\n",
    "\n",
    "        if fermatas is not None:\n",
    "            tensor_metadata = self.dataset.set_fermatas(tensor_metadata,\n",
    "                                                        fermatas)\n",
    "\n",
    "        # timesteps_ticks is the number of ticks on which we unroll the LSTMs\n",
    "        # it is also the padding size\n",
    "        timesteps_ticks = self.dataset.sequences_size * self.dataset.subdivision // 2\n",
    "        if time_index_range_ticks is None:\n",
    "            time_index_range_ticks = [timesteps_ticks, sequence_length_ticks + timesteps_ticks]\n",
    "        else:\n",
    "            a_ticks, b_ticks = time_index_range_ticks\n",
    "            assert 0 <= a_ticks < b_ticks <= sequence_length_ticks\n",
    "            time_index_range_ticks = [a_ticks + timesteps_ticks, b_ticks + timesteps_ticks]\n",
    "\n",
    "        if voice_index_range is None:\n",
    "            voice_index_range = [0, self.dataset.num_voices]\n",
    "\n",
    "        tensor_chorale = self.dataset.extract_score_tensor_with_padding(\n",
    "            tensor_score=tensor_chorale,\n",
    "            start_tick=-timesteps_ticks,\n",
    "            end_tick=sequence_length_ticks + timesteps_ticks\n",
    "        )\n",
    "\n",
    "        tensor_metadata_padded = self.dataset.extract_metadata_with_padding(\n",
    "            tensor_metadata=tensor_metadata,\n",
    "            start_tick=-timesteps_ticks,\n",
    "            end_tick=sequence_length_ticks + timesteps_ticks\n",
    "        )\n",
    "\n",
    "        # randomize regenerated part\n",
    "        if random_init:\n",
    "            a, b = time_index_range_ticks\n",
    "            tensor_chorale[voice_index_range[0]:voice_index_range[1], a:b] = self.dataset.random_score_tensor(\n",
    "                b - a)[voice_index_range[0]:voice_index_range[1], :]\n",
    "\n",
    "        tensor_chorale = self.parallel_gibbs(\n",
    "            tensor_chorale=tensor_chorale,\n",
    "            tensor_metadata=tensor_metadata_padded,\n",
    "            num_iterations=num_iterations,\n",
    "            timesteps_ticks=timesteps_ticks,\n",
    "            temperature=temperature,\n",
    "            batch_size_per_voice=batch_size_per_voice,\n",
    "            time_index_range_ticks=time_index_range_ticks,\n",
    "            voice_index_range=voice_index_range,\n",
    "        )\n",
    "\n",
    "        # get fermata tensor\n",
    "        for metadata_index, metadata in enumerate(self.dataset.metadatas):\n",
    "            if isinstance(metadata, FermataMetadata):\n",
    "                break\n",
    "\n",
    "\n",
    "        score = self.dataset.tensor_to_score(\n",
    "            tensor_score=tensor_chorale,\n",
    "            fermata_tensor=tensor_metadata[:, :, metadata_index])\n",
    "\n",
    "        return score, tensor_chorale, tensor_metadata\n",
    "\n",
    "    def parallel_gibbs(self,\n",
    "                       tensor_chorale,\n",
    "                       tensor_metadata,\n",
    "                       timesteps_ticks,\n",
    "                       num_iterations=1000,\n",
    "                       batch_size_per_voice=16,\n",
    "                       temperature=1.,\n",
    "                       time_index_range_ticks=None,\n",
    "                       voice_index_range=None,\n",
    "                       ):\n",
    "        \"\"\"\n",
    "        Parallel pseudo-Gibbs sampling\n",
    "        tensor_chorale and tensor_metadata are padded with\n",
    "        timesteps_ticks START_SYMBOLS before,\n",
    "        timesteps_ticks END_SYMBOLS after\n",
    "        :param tensor_chorale: (num_voices, chorale_length) tensor\n",
    "        :param tensor_metadata: (num_voices, chorale_length) tensor\n",
    "        :param timesteps_ticks:\n",
    "        :param num_iterations: number of Gibbs sampling iterations\n",
    "        :param batch_size_per_voice: number of simultaneous parallel updates\n",
    "        :param temperature: final temperature after simulated annealing\n",
    "        :param time_index_range_ticks: list of two integers [a, b] or None; can be used \\\n",
    "        to regenerate only the portion of the score between timesteps a and b\n",
    "        :param voice_index_range: list of two integers [a, b] or None; can be used \\\n",
    "        to regenerate only the portion of the score between voice_index a and b\n",
    "        :return: (num_voices, chorale_length) tensor\n",
    "        \"\"\"\n",
    "        start_voice, end_voice = voice_index_range\n",
    "        # add batch_dimension\n",
    "        tensor_chorale = tensor_chorale.unsqueeze(0)\n",
    "        tensor_chorale_no_cuda = tensor_chorale.clone()\n",
    "        tensor_metadata = tensor_metadata.unsqueeze(0)\n",
    "\n",
    "        # to variable\n",
    "        tensor_chorale = cuda_variable(tensor_chorale)\n",
    "        tensor_metadata = cuda_variable(tensor_metadata)\n",
    "\n",
    "        min_temperature = temperature\n",
    "        temperature = 1.1\n",
    "\n",
    "        # Main loop\n",
    "        for iteration in tqdm(range(num_iterations)):\n",
    "            # annealing\n",
    "            temperature = max(min_temperature, temperature * 0.9993)\n",
    "            # print(temperature)\n",
    "            time_indexes_ticks = {}\n",
    "            probas = {}\n",
    "\n",
    "            for voice_index in range(start_voice, end_voice):\n",
    "                batch_notes = []\n",
    "                batch_metas = []\n",
    "\n",
    "                time_indexes_ticks[voice_index] = []\n",
    "\n",
    "                # create batches of inputs\n",
    "                for batch_index in range(batch_size_per_voice):\n",
    "                    time_index_ticks = np.random.randint(\n",
    "                        *time_index_range_ticks)\n",
    "                    time_indexes_ticks[voice_index].append(time_index_ticks)\n",
    "\n",
    "                    notes, label = (self.voice_models[voice_index]\n",
    "                                    .preprocess_notes(\n",
    "                            tensor_chorale=tensor_chorale[\n",
    "                                           :, :,\n",
    "                                           time_index_ticks - timesteps_ticks:\n",
    "                                           time_index_ticks + timesteps_ticks],\n",
    "                            time_index_ticks=timesteps_ticks\n",
    "                        )\n",
    "                    )\n",
    "                    metas = self.voice_models[voice_index].preprocess_metas(\n",
    "                        tensor_metadata=tensor_metadata[\n",
    "                                        :, :,\n",
    "                                        time_index_ticks - timesteps_ticks:\n",
    "                                        time_index_ticks + timesteps_ticks,\n",
    "                                        :],\n",
    "                        time_index_ticks=timesteps_ticks\n",
    "                    )\n",
    "\n",
    "                    batch_notes.append(notes)\n",
    "                    batch_metas.append(metas)\n",
    "\n",
    "                # reshape batches\n",
    "                batch_notes = list(map(list, zip(*batch_notes)))\n",
    "                batch_notes = [torch.cat(lcr) if lcr[0] is not None else None\n",
    "                               for lcr in batch_notes]\n",
    "                batch_metas = list(map(list, zip(*batch_metas)))\n",
    "                batch_metas = [torch.cat(lcr)\n",
    "                               for lcr in batch_metas]\n",
    "\n",
    "                # make all estimations\n",
    "                probas[voice_index] = (self.voice_models[voice_index]\n",
    "                                       .forward(batch_notes, batch_metas)\n",
    "                                       )\n",
    "                probas[voice_index] = nn.Softmax(dim=1)(probas[voice_index])\n",
    "\n",
    "            # update all predictions\n",
    "            for voice_index in range(start_voice, end_voice):\n",
    "                for batch_index in range(batch_size_per_voice):\n",
    "                    probas_pitch = probas[voice_index][batch_index]\n",
    "\n",
    "                    probas_pitch = to_numpy(probas_pitch)\n",
    "\n",
    "                    # use temperature\n",
    "                    probas_pitch = np.log(probas_pitch) / temperature\n",
    "                    probas_pitch = np.exp(probas_pitch) / np.sum(\n",
    "                        np.exp(probas_pitch)) - 1e-7\n",
    "\n",
    "                    # avoid non-probabilities\n",
    "                    probas_pitch[probas_pitch < 0] = 0\n",
    "\n",
    "                    # pitch can include slur_symbol\n",
    "                    pitch = np.argmax(np.random.multinomial(1, probas_pitch))\n",
    "\n",
    "                    tensor_chorale_no_cuda[\n",
    "                        0,\n",
    "                        voice_index,\n",
    "                        time_indexes_ticks[voice_index][batch_index]\n",
    "                    ] = int(pitch)\n",
    "\n",
    "            tensor_chorale = cuda_variable(tensor_chorale_no_cuda.clone())\n",
    "\n",
    "        return tensor_chorale_no_cuda[0, :, timesteps_ticks:-timesteps_ticks]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6714008",
   "metadata": {
    "id": "a6714008"
   },
   "source": [
    "## Training and Generation\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5fbb3b25",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5fbb3b25",
    "outputId": "9f5e2e99-2a36-42bf-b852-8c06bd2a0243"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading ChoraleDataset([0, 1, 2, 3],bach_chorales,['fermata', 'tick', 'key'],8,4) from /home/mylh/Projects/music_ai/DreamBach/dataset_cache/datasets/ChoraleDataset([0, 1, 2, 3],bach_chorales,['fermata', 'tick', 'key'],8,4)\n",
      "(the corresponding TensorDataset is not loaded)\n"
     ]
    }
   ],
   "source": [
    "dataset_manager = DatasetManager()\n",
    "\n",
    "metadatas = [\n",
    "   FermataMetadata(),\n",
    "   TickMetadata(subdivision=4),\n",
    "   KeyMetadata()\n",
    "]\n",
    "chorale_dataset_kwargs = {\n",
    "    'voice_ids':      [0, 1, 2, 3],\n",
    "    'metadatas':      metadatas,\n",
    "    'sequences_size': 8,\n",
    "    'subdivision':    4\n",
    "}\n",
    "dataset = dataset_manager.get_dataset(\n",
    "    name='bach_chorales',\n",
    "    **chorale_dataset_kwargs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2e8ef51c",
   "metadata": {
    "id": "2e8ef51c"
   },
   "outputs": [],
   "source": [
    "# size of the note embeddings\n",
    "note_embedding_dim = 20\n",
    "# size of the metadata embeddings\n",
    "meta_embedding_dim = 20\n",
    "# number of layers of the LSTMs\n",
    "num_layers = 2\n",
    "# hidden size of the LSTMs\n",
    "lstm_hidden_size = 256\n",
    "# amount of dropout between LSTM layers\n",
    "dropout_lstm = 0.5\n",
    "#hidden size of the Linear layers\n",
    "linear_hidden_size = 256\n",
    "\n",
    "deepbach = DeepBach(\n",
    "    dataset=dataset,\n",
    "    note_embedding_dim=note_embedding_dim,\n",
    "    meta_embedding_dim=meta_embedding_dim,\n",
    "    num_layers=num_layers,\n",
    "    lstm_hidden_size=lstm_hidden_size,\n",
    "    dropout_lstm=dropout_lstm,\n",
    "    linear_hidden_size=linear_hidden_size\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8435e803",
   "metadata": {
    "id": "8435e803"
   },
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "66498f1e",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "66498f1e",
    "outputId": "103d650c-ef27-4687-c651-ebdae02f9156"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===Epoch 0===\n",
      "Loading TensorDataset for ChoraleDataset([0, 1, 2, 3],bach_chorales,['fermata', 'tick', 'key'],8,4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 884/884 [02:34<00:00,  5.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 0.4498386669785998\n",
      "Training accuracy: 88.10825261595022\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 104/104 [00:13<00:00,  7.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss: 0.2931521643096438\n",
      "Validation accuracy: 91.30108173076923\n",
      "Model VoiceModel(ChoraleDataset([0, 1, 2, 3],bach_chorales,['fermata', 'tick', 'key'],8,4),0,20,20,2,256,0.5,256) saved\n",
      "===Epoch 1===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 884/884 [02:59<00:00,  4.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 0.20248120912767914\n",
      "Training accuracy: 93.65897200226244\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 104/104 [00:12<00:00,  8.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss: 0.16356901040014166\n",
      "Validation accuracy: 94.64768629807692\n",
      "Model VoiceModel(ChoraleDataset([0, 1, 2, 3],bach_chorales,['fermata', 'tick', 'key'],8,4),0,20,20,2,256,0.5,256) saved\n",
      "===Epoch 2===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 884/884 [02:59<00:00,  4.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 0.13681844640438912\n",
      "Training accuracy: 95.51300463093891\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 104/104 [00:12<00:00,  8.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss: 0.13190664884705955\n",
      "Validation accuracy: 95.54725060096153\n",
      "Model VoiceModel(ChoraleDataset([0, 1, 2, 3],bach_chorales,['fermata', 'tick', 'key'],8,4),0,20,20,2,256,0.5,256) saved\n",
      "===Epoch 3===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 884/884 [03:01<00:00,  4.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 0.1143156091063136\n",
      "Training accuracy: 96.19770308964932\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 104/104 [00:13<00:00,  7.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss: 0.1208761902884222\n",
      "Validation accuracy: 95.96604567307692\n",
      "Model VoiceModel(ChoraleDataset([0, 1, 2, 3],bach_chorales,['fermata', 'tick', 'key'],8,4),0,20,20,2,256,0.5,256) saved\n",
      "===Epoch 4===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 884/884 [02:59<00:00,  4.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 0.09725209705666449\n",
      "Training accuracy: 96.74199307126698\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 104/104 [00:12<00:00,  8.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss: 0.11310557138103132\n",
      "Validation accuracy: 96.23741736778847\n",
      "Model VoiceModel(ChoraleDataset([0, 1, 2, 3],bach_chorales,['fermata', 'tick', 'key'],8,4),0,20,20,2,256,0.5,256) saved\n",
      "===Epoch 0===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 884/884 [02:59<00:00,  4.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 0.5630920218868493\n",
      "Training accuracy: 85.46744644372171\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 104/104 [00:12<00:00,  8.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss: 0.33286128064187676\n",
      "Validation accuracy: 89.74139873798077\n",
      "Model VoiceModel(ChoraleDataset([0, 1, 2, 3],bach_chorales,['fermata', 'tick', 'key'],8,4),1,20,20,2,256,0.5,256) saved\n",
      "===Epoch 1===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 884/884 [03:00<00:00,  4.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 0.2584828424406537\n",
      "Training accuracy: 91.87398366798642\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 104/104 [00:13<00:00,  7.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss: 0.19953413830640224\n",
      "Validation accuracy: 93.59975961538461\n",
      "Model VoiceModel(ChoraleDataset([0, 1, 2, 3],bach_chorales,['fermata', 'tick', 'key'],8,4),1,20,20,2,256,0.5,256) saved\n",
      "===Epoch 2===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 884/884 [02:59<00:00,  4.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 0.1769084606100531\n",
      "Training accuracy: 94.30412188914028\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 104/104 [00:12<00:00,  8.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss: 0.17039434370011663\n",
      "Validation accuracy: 94.58477313701923\n",
      "Model VoiceModel(ChoraleDataset([0, 1, 2, 3],bach_chorales,['fermata', 'tick', 'key'],8,4),1,20,20,2,256,0.5,256) saved\n",
      "===Epoch 3===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 884/884 [02:59<00:00,  4.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 0.1459686601475488\n",
      "Training accuracy: 95.25350855486425\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 104/104 [00:12<00:00,  8.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss: 0.15352978851073062\n",
      "Validation accuracy: 95.08056640625\n",
      "Model VoiceModel(ChoraleDataset([0, 1, 2, 3],bach_chorales,['fermata', 'tick', 'key'],8,4),1,20,20,2,256,0.5,256) saved\n",
      "===Epoch 4===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 884/884 [02:59<00:00,  4.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 0.12757344837237267\n",
      "Training accuracy: 95.79735665299773\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 104/104 [00:13<00:00,  7.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss: 0.14907322393264621\n",
      "Validation accuracy: 95.24583082932692\n",
      "Model VoiceModel(ChoraleDataset([0, 1, 2, 3],bach_chorales,['fermata', 'tick', 'key'],8,4),1,20,20,2,256,0.5,256) saved\n",
      "===Epoch 0===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 884/884 [02:59<00:00,  4.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 0.5848678592508195\n",
      "Training accuracy: 84.8399718962104\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 104/104 [00:13<00:00,  7.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss: 0.37076616974977344\n",
      "Validation accuracy: 88.74887319711539\n",
      "Model VoiceModel(ChoraleDataset([0, 1, 2, 3],bach_chorales,['fermata', 'tick', 'key'],8,4),2,20,20,2,256,0.5,256) saved\n",
      "===Epoch 1===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 884/884 [02:59<00:00,  4.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 0.28195832986637476\n",
      "Training accuracy: 91.23369449943439\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 104/104 [00:13<00:00,  7.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss: 0.2193393908226146\n",
      "Validation accuracy: 93.3124248798077\n",
      "Model VoiceModel(ChoraleDataset([0, 1, 2, 3],bach_chorales,['fermata', 'tick', 'key'],8,4),2,20,20,2,256,0.5,256) saved\n",
      "===Epoch 2===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 884/884 [02:59<00:00,  4.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 0.19690251395072603\n",
      "Training accuracy: 93.69487503535068\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 104/104 [00:12<00:00,  8.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss: 0.18788440418071473\n",
      "Validation accuracy: 94.21762319711539\n",
      "Model VoiceModel(ChoraleDataset([0, 1, 2, 3],bach_chorales,['fermata', 'tick', 'key'],8,4),2,20,20,2,256,0.5,256) saved\n",
      "===Epoch 3===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 884/884 [02:59<00:00,  4.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 0.1626171137025049\n",
      "Training accuracy: 94.72269601951358\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 104/104 [00:12<00:00,  8.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss: 0.1752836366649717\n",
      "Validation accuracy: 94.68994140625\n",
      "Model VoiceModel(ChoraleDataset([0, 1, 2, 3],bach_chorales,['fermata', 'tick', 'key'],8,4),2,20,20,2,256,0.5,256) saved\n",
      "===Epoch 4===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 884/884 [02:59<00:00,  4.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 0.13903965107354913\n",
      "Training accuracy: 95.41976721578054\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 104/104 [00:13<00:00,  7.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss: 0.164635705958622\n",
      "Validation accuracy: 94.99417818509616\n",
      "Model VoiceModel(ChoraleDataset([0, 1, 2, 3],bach_chorales,['fermata', 'tick', 'key'],8,4),2,20,20,2,256,0.5,256) saved\n",
      "===Epoch 0===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 884/884 [02:59<00:00,  4.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 0.5980385771501657\n",
      "Training accuracy: 84.42526424632354\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 104/104 [00:13<00:00,  7.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss: 0.3295364680055242\n",
      "Validation accuracy: 89.92074819711539\n",
      "Model VoiceModel(ChoraleDataset([0, 1, 2, 3],bach_chorales,['fermata', 'tick', 'key'],8,4),3,20,20,2,256,0.5,256) saved\n",
      "===Epoch 1===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 884/884 [02:59<00:00,  4.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 0.23807258287031727\n",
      "Training accuracy: 92.48775982748869\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 104/104 [00:12<00:00,  8.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss: 0.20373624982312322\n",
      "Validation accuracy: 93.61666165865384\n",
      "Model VoiceModel(ChoraleDataset([0, 1, 2, 3],bach_chorales,['fermata', 'tick', 'key'],8,4),3,20,20,2,256,0.5,256) saved\n",
      "===Epoch 2===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 884/884 [03:00<00:00,  4.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 0.16525855560985087\n",
      "Training accuracy: 94.67033282664028\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 104/104 [00:13<00:00,  7.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss: 0.16700757948610073\n",
      "Validation accuracy: 94.70966045673077\n",
      "Model VoiceModel(ChoraleDataset([0, 1, 2, 3],bach_chorales,['fermata', 'tick', 'key'],8,4),3,20,20,2,256,0.5,256) saved\n",
      "===Epoch 3===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 884/884 [02:59<00:00,  4.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 0.13269692773159542\n",
      "Training accuracy: 95.673297864819\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 104/104 [00:12<00:00,  8.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss: 0.14987408160232008\n",
      "Validation accuracy: 95.36226712740384\n",
      "Model VoiceModel(ChoraleDataset([0, 1, 2, 3],bach_chorales,['fermata', 'tick', 'key'],8,4),3,20,20,2,256,0.5,256) saved\n",
      "===Epoch 4===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 884/884 [02:59<00:00,  4.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 0.11149386306423947\n",
      "Training accuracy: 96.30817396069004\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 104/104 [00:13<00:00,  7.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss: 0.14342686001103944\n",
      "Validation accuracy: 95.55006760817308\n",
      "Model VoiceModel(ChoraleDataset([0, 1, 2, 3],bach_chorales,['fermata', 'tick', 'key'],8,4),3,20,20,2,256,0.5,256) saved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "batch_size = 1024\n",
    "num_epochs = 5\n",
    "deepbach.train(batch_size=batch_size, num_epochs=num_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba24749e",
   "metadata": {
    "id": "ba24749e"
   },
   "source": [
    "## Generate using trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b88ef671",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 222
    },
    "id": "b88ef671",
    "outputId": "b90da937-b707-42c9-b180-b345103eed41"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading VoiceModel(ChoraleDataset([0, 1, 2, 3],bach_chorales,['fermata', 'tick', 'key'],8,4),0,20,20,2,256,0.5,256)\n",
      "Loading VoiceModel(ChoraleDataset([0, 1, 2, 3],bach_chorales,['fermata', 'tick', 'key'],8,4),1,20,20,2,256,0.5,256)\n",
      "Loading VoiceModel(ChoraleDataset([0, 1, 2, 3],bach_chorales,['fermata', 'tick', 'key'],8,4),2,20,20,2,256,0.5,256)\n",
      "Loading VoiceModel(ChoraleDataset([0, 1, 2, 3],bach_chorales,['fermata', 'tick', 'key'],8,4),3,20,20,2,256,0.5,256)\n",
      "Generation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 500/500 [00:12<00:00, 40.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated file saved into /home/mylh/Projects/music_ai/DreamBach/midi/2023-06-11T08:44:13_500_256.mid\n"
     ]
    }
   ],
   "source": [
    "# number of parallel pseudo-Gibbs sampling iterations\n",
    "num_iterations = 500\n",
    "\n",
    "# length of the generated chorale (in ticks)\n",
    "sequence_length_ticks = 64 * 4\n",
    "\n",
    "deepbach.load()\n",
    "deepbach.cuda()\n",
    "\n",
    "print('Generation')\n",
    "score, tensor_chorale, tensor_metadata = deepbach.generation(\n",
    "    num_iterations=num_iterations,\n",
    "    sequence_length_ticks=sequence_length_ticks,\n",
    ")\n",
    "#score.show('txt')\n",
    "#score.show()\n",
    "name = f\"{dt.datetime.now().isoformat()[:-7]}_{num_iterations}_{sequence_length_ticks}.mid\"\n",
    "mf = music21.midi.translate.streamToMidiFile(score)\n",
    "midi_file_path = os.path.join(MIDI_SAVE_DIR, name)\n",
    "with open(midi_file_path, 'wb') as midi_file:\n",
    "    mf.openFileLike(midi_file)\n",
    "    mf.write()\n",
    "    mf.close()\n",
    "\n",
    "#audio = Audio(filename=midi_file_path)\n",
    "#display(audio)\n",
    "print(f\"Generated file saved into {midi_file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39626348",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "_yDYVjHYrDZr",
    "Cx_IMqN_UJfz",
    "8435e803"
   ],
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
